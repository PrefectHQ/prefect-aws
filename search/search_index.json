{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"prefect-aws Welcome! prefect-aws is a collection of pre-built Prefect tasks that can be used to quickly construct Prefect flows that interact with Amazon Web Services. Getting Started Python setup Requires an installation of Python 3.7+ We recommend using a Python virtual environment manager such as pipenv, conda or virtualenv. These tasks are designed to work with Prefect 2.0. For more information about how to use Prefect, please refer to the Prefect documentation . Installation Install prefect-aws pip install prefect-aws AWS Authentication You will need to obtain AWS credentials in order to use these tasks. Refer to the AWS documentation for authentication methods available. Write and run a flow with prefect-aws tasks from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.s3 import s3_upload @flow def example_s3_upload_flow (): aws_credentials = AwsCredentials ( aws_access_key_id = \"acccess_key_id\" , aws_secret_access_key = \"secret_access_key\" ) with open ( \"data.csv\" , \"rb\" ) as file : key = s3_upload ( bucket = \"bucket\" , key = \"data.csv\" , data = file . read (), aws_credentials = aws_credentials , ) example_s3_upload_flow () Write and run a flow with AwsCredentials and S3Bucket import asyncio from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.s3 import S3Bucket @flow async def aws_s3_bucket_roundtrip (): # create an AwsCredentials block here or through UI aws_creds = AwsCredentials ( aws_access_key_id = \"AWS_ACCESS_KEY_ID\" , aws_secret_access_key = \"AWS_SECRET_ACCESS_KEY\" ) s3_bucket = S3Bucket ( bucket_name = \"bucket\" , # must exist aws_credentials = aws_creds , basepath = \"subfolder\" , ) key = await s3_bucket . write_path ( \"data.csv\" , content = b \"hello\" ) return await s3_bucket . read_path ( key ) asyncio . run ( aws_s3_bucket_roundtrip ()) Write and run an async flow by loading a MinIOCredentials block to use in S3Bucket import asyncio from prefect import flow from prefect_aws import MinIOCredentials from prefect_aws.s3 import S3Bucket @flow async def minio_s3_bucket_roundtrip (): minio_creds = MinIOCredentials . load ( \"MY_BLOCK_NAME\" ) s3_bucket = S3Bucket ( bucket_name = \"bucket\" , # must exist minio_credentials = minio_creds , endpoint_url = \"http://localhost:9000\" ) path_to_file = await s3_bucket . write_path ( \"/data.csv\" , content = b \"hello\" ) return await s3_bucket . read_path ( path_to_file ) asyncio . run ( minio_s3_bucket_roundtrip ()) Next steps Refer to the API documentation in the side menu to explore all the capabilities of Prefect AWS! Resources If you encounter and bugs while using prefect-aws , feel free to open an issue in the prefect-aws repository. If you have any questions or issues while using prefect-aws , you can find help in either the Prefect Discourse forum or the Prefect Slack community Development If you'd like to install a version of prefect-aws for development, first clone the repository and then perform an editable install with pip : git clone https://github.com/PrefectHQ/prefect-aws.git cd prefect-aws/ pip install -e \".[dev]\"","title":"Home"},{"location":"#prefect-aws","text":"","title":"prefect-aws"},{"location":"#welcome","text":"prefect-aws is a collection of pre-built Prefect tasks that can be used to quickly construct Prefect flows that interact with Amazon Web Services.","title":"Welcome!"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#python-setup","text":"Requires an installation of Python 3.7+ We recommend using a Python virtual environment manager such as pipenv, conda or virtualenv. These tasks are designed to work with Prefect 2.0. For more information about how to use Prefect, please refer to the Prefect documentation .","title":"Python setup"},{"location":"#installation","text":"Install prefect-aws pip install prefect-aws","title":"Installation"},{"location":"#aws-authentication","text":"You will need to obtain AWS credentials in order to use these tasks. Refer to the AWS documentation for authentication methods available.","title":"AWS Authentication"},{"location":"#write-and-run-a-flow-with-prefect-aws-tasks","text":"from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.s3 import s3_upload @flow def example_s3_upload_flow (): aws_credentials = AwsCredentials ( aws_access_key_id = \"acccess_key_id\" , aws_secret_access_key = \"secret_access_key\" ) with open ( \"data.csv\" , \"rb\" ) as file : key = s3_upload ( bucket = \"bucket\" , key = \"data.csv\" , data = file . read (), aws_credentials = aws_credentials , ) example_s3_upload_flow ()","title":"Write and run a flow with prefect-aws tasks"},{"location":"#write-and-run-a-flow-with-awscredentials-and-s3bucket","text":"import asyncio from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.s3 import S3Bucket @flow async def aws_s3_bucket_roundtrip (): # create an AwsCredentials block here or through UI aws_creds = AwsCredentials ( aws_access_key_id = \"AWS_ACCESS_KEY_ID\" , aws_secret_access_key = \"AWS_SECRET_ACCESS_KEY\" ) s3_bucket = S3Bucket ( bucket_name = \"bucket\" , # must exist aws_credentials = aws_creds , basepath = \"subfolder\" , ) key = await s3_bucket . write_path ( \"data.csv\" , content = b \"hello\" ) return await s3_bucket . read_path ( key ) asyncio . run ( aws_s3_bucket_roundtrip ())","title":"Write and run a flow with AwsCredentials and S3Bucket"},{"location":"#write-and-run-an-async-flow-by-loading-a-miniocredentials-block-to-use-in-s3bucket","text":"import asyncio from prefect import flow from prefect_aws import MinIOCredentials from prefect_aws.s3 import S3Bucket @flow async def minio_s3_bucket_roundtrip (): minio_creds = MinIOCredentials . load ( \"MY_BLOCK_NAME\" ) s3_bucket = S3Bucket ( bucket_name = \"bucket\" , # must exist minio_credentials = minio_creds , endpoint_url = \"http://localhost:9000\" ) path_to_file = await s3_bucket . write_path ( \"/data.csv\" , content = b \"hello\" ) return await s3_bucket . read_path ( path_to_file ) asyncio . run ( minio_s3_bucket_roundtrip ())","title":"Write and run an async flow by loading a MinIOCredentials block to use in S3Bucket"},{"location":"#next-steps","text":"Refer to the API documentation in the side menu to explore all the capabilities of Prefect AWS!","title":"Next steps"},{"location":"#resources","text":"If you encounter and bugs while using prefect-aws , feel free to open an issue in the prefect-aws repository. If you have any questions or issues while using prefect-aws , you can find help in either the Prefect Discourse forum or the Prefect Slack community","title":"Resources"},{"location":"#development","text":"If you'd like to install a version of prefect-aws for development, first clone the repository and then perform an editable install with pip : git clone https://github.com/PrefectHQ/prefect-aws.git cd prefect-aws/ pip install -e \".[dev]\"","title":"Development"},{"location":"batch/","text":"prefect_aws.batch Tasks for interacting with AWS Batch batch_submit async Submit a job to the AWS Batch job service. Parameters: Name Type Description Default job_name str The AWS batch job name. required job_definition str The AWS batch job definition. required job_queue str Name of the AWS batch job queue. required aws_credentials AwsCredentials Credentials to use for authentication with AWS. required **batch_kwargs Optional [ Dict [ str , Any ]] Additional keyword arguments to pass to the boto3 submit_job function. See the documentation for submit_job for more details. {} Returns: Type Description The id corresponding to the job. Example Submits a job to batch. from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.batch import batch_submit @flow def example_batch_submit_flow (): aws_credentials = AwsCredentials ( aws_access_key_id = \"acccess_key_id\" , aws_secret_access_key = \"secret_access_key\" ) job_id = batch_submit ( \"job_name\" , \"job_definition\" , \"job_queue\" , aws_credentials ) return job_id example_batch_submit_flow () Source code in prefect_aws/batch.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 @task async def batch_submit ( job_name : str , job_queue : str , job_definition : str , aws_credentials : AwsCredentials , ** batch_kwargs : Optional [ Dict [ str , Any ]], ): \"\"\" Submit a job to the AWS Batch job service. Args: job_name: The AWS batch job name. job_definition: The AWS batch job definition. job_queue: Name of the AWS batch job queue. aws_credentials: Credentials to use for authentication with AWS. **batch_kwargs: Additional keyword arguments to pass to the boto3 `submit_job` function. See the documentation for [submit_job](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/batch.html#Batch.Client.submit_job) for more details. Returns: The id corresponding to the job. Example: Submits a job to batch. ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.batch import batch_submit @flow def example_batch_submit_flow(): aws_credentials = AwsCredentials( aws_access_key_id=\"acccess_key_id\", aws_secret_access_key=\"secret_access_key\" ) job_id = batch_submit( \"job_name\", \"job_definition\", \"job_queue\", aws_credentials ) return job_id example_batch_submit_flow() ``` \"\"\" # noqa logger = get_run_logger () logger . info ( \"Preparing to submit %s job to %s job queue\" , job_name , job_queue ) batch_client = aws_credentials . get_boto3_session () . client ( \"batch\" ) response = await run_sync_in_worker_thread ( batch_client . submit_job , jobName = job_name , jobQueue = job_queue , jobDefinition = job_definition , ** batch_kwargs , ) return response [ \"jobId\" ]","title":"Batch"},{"location":"batch/#prefect_aws.batch","text":"Tasks for interacting with AWS Batch","title":"batch"},{"location":"batch/#prefect_aws.batch.batch_submit","text":"Submit a job to the AWS Batch job service. Parameters: Name Type Description Default job_name str The AWS batch job name. required job_definition str The AWS batch job definition. required job_queue str Name of the AWS batch job queue. required aws_credentials AwsCredentials Credentials to use for authentication with AWS. required **batch_kwargs Optional [ Dict [ str , Any ]] Additional keyword arguments to pass to the boto3 submit_job function. See the documentation for submit_job for more details. {} Returns: Type Description The id corresponding to the job. Example Submits a job to batch. from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.batch import batch_submit @flow def example_batch_submit_flow (): aws_credentials = AwsCredentials ( aws_access_key_id = \"acccess_key_id\" , aws_secret_access_key = \"secret_access_key\" ) job_id = batch_submit ( \"job_name\" , \"job_definition\" , \"job_queue\" , aws_credentials ) return job_id example_batch_submit_flow () Source code in prefect_aws/batch.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 @task async def batch_submit ( job_name : str , job_queue : str , job_definition : str , aws_credentials : AwsCredentials , ** batch_kwargs : Optional [ Dict [ str , Any ]], ): \"\"\" Submit a job to the AWS Batch job service. Args: job_name: The AWS batch job name. job_definition: The AWS batch job definition. job_queue: Name of the AWS batch job queue. aws_credentials: Credentials to use for authentication with AWS. **batch_kwargs: Additional keyword arguments to pass to the boto3 `submit_job` function. See the documentation for [submit_job](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/batch.html#Batch.Client.submit_job) for more details. Returns: The id corresponding to the job. Example: Submits a job to batch. ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.batch import batch_submit @flow def example_batch_submit_flow(): aws_credentials = AwsCredentials( aws_access_key_id=\"acccess_key_id\", aws_secret_access_key=\"secret_access_key\" ) job_id = batch_submit( \"job_name\", \"job_definition\", \"job_queue\", aws_credentials ) return job_id example_batch_submit_flow() ``` \"\"\" # noqa logger = get_run_logger () logger . info ( \"Preparing to submit %s job to %s job queue\" , job_name , job_queue ) batch_client = aws_credentials . get_boto3_session () . client ( \"batch\" ) response = await run_sync_in_worker_thread ( batch_client . submit_job , jobName = job_name , jobQueue = job_queue , jobDefinition = job_definition , ** batch_kwargs , ) return response [ \"jobId\" ]","title":"batch_submit()"},{"location":"client_waiter/","text":"prefect_aws.client_waiter Task for waiting on a long-running AWS job client_waiter async Uses the underlying boto3 waiter functionality. Parameters: Name Type Description Default client str The AWS client on which to wait (e.g., 'client_wait', 'ec2', etc). required waiter_name str The name of the waiter to instantiate. You may also use a custom waiter name, if you supply an accompanying waiter definition dict. required aws_credentials AwsCredentials Credentials to use for authentication with AWS. required waiter_definition Optional [ Dict [ str , Any ]] A valid custom waiter model, as a dict. Note that if you supply a custom definition, it is assumed that the provided 'waiter_name' is contained within the waiter definition dict. None **waiter_kwargs Optional [ Dict [ str , Any ]] Arguments to pass to the waiter.wait(...) method. Will depend upon the specific waiter being called. {} Example Run an ec2 waiter until instance_exists. from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.client_wait import client_waiter @flow def example_client_wait_flow (): aws_credentials = AwsCredentials ( aws_access_key_id = \"acccess_key_id\" , aws_secret_access_key = \"secret_access_key\" ) waiter = client_waiter ( \"ec2\" , \"instance_exists\" , aws_credentials ) return waiter example_client_wait_flow () Source code in prefect_aws/client_waiter.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 @task async def client_waiter ( client : str , waiter_name : str , aws_credentials : AwsCredentials , waiter_definition : Optional [ Dict [ str , Any ]] = None , ** waiter_kwargs : Optional [ Dict [ str , Any ]], ): \"\"\" Uses the underlying boto3 waiter functionality. Args: client: The AWS client on which to wait (e.g., 'client_wait', 'ec2', etc). waiter_name: The name of the waiter to instantiate. You may also use a custom waiter name, if you supply an accompanying waiter definition dict. aws_credentials: Credentials to use for authentication with AWS. waiter_definition: A valid custom waiter model, as a dict. Note that if you supply a custom definition, it is assumed that the provided 'waiter_name' is contained within the waiter definition dict. **waiter_kwargs: Arguments to pass to the `waiter.wait(...)` method. Will depend upon the specific waiter being called. Example: Run an ec2 waiter until instance_exists. ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.client_wait import client_waiter @flow def example_client_wait_flow(): aws_credentials = AwsCredentials( aws_access_key_id=\"acccess_key_id\", aws_secret_access_key=\"secret_access_key\" ) waiter = client_waiter( \"ec2\", \"instance_exists\", aws_credentials ) return waiter example_client_wait_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Waiting on %s job\" , client ) boto_client = aws_credentials . get_boto3_session () . client ( client ) if waiter_definition is not None : # Use user-provided waiter definition waiter_model = WaiterModel ( waiter_definition ) waiter = create_waiter_with_client ( waiter_name , waiter_model , boto_client ) elif waiter_name in boto_client . waiter_names : waiter = boto_client . get_waiter ( waiter_name ) else : raise ValueError ( f \"The waiter name, { waiter_name } , is not a valid boto waiter; \" f \"if using a custom waiter, you must provide a waiter definition\" ) await run_sync_in_worker_thread ( waiter . wait , ** waiter_kwargs )","title":"Client Waiter"},{"location":"client_waiter/#prefect_aws.client_waiter","text":"Task for waiting on a long-running AWS job","title":"client_waiter"},{"location":"client_waiter/#prefect_aws.client_waiter.client_waiter","text":"Uses the underlying boto3 waiter functionality. Parameters: Name Type Description Default client str The AWS client on which to wait (e.g., 'client_wait', 'ec2', etc). required waiter_name str The name of the waiter to instantiate. You may also use a custom waiter name, if you supply an accompanying waiter definition dict. required aws_credentials AwsCredentials Credentials to use for authentication with AWS. required waiter_definition Optional [ Dict [ str , Any ]] A valid custom waiter model, as a dict. Note that if you supply a custom definition, it is assumed that the provided 'waiter_name' is contained within the waiter definition dict. None **waiter_kwargs Optional [ Dict [ str , Any ]] Arguments to pass to the waiter.wait(...) method. Will depend upon the specific waiter being called. {} Example Run an ec2 waiter until instance_exists. from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.client_wait import client_waiter @flow def example_client_wait_flow (): aws_credentials = AwsCredentials ( aws_access_key_id = \"acccess_key_id\" , aws_secret_access_key = \"secret_access_key\" ) waiter = client_waiter ( \"ec2\" , \"instance_exists\" , aws_credentials ) return waiter example_client_wait_flow () Source code in prefect_aws/client_waiter.py 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 @task async def client_waiter ( client : str , waiter_name : str , aws_credentials : AwsCredentials , waiter_definition : Optional [ Dict [ str , Any ]] = None , ** waiter_kwargs : Optional [ Dict [ str , Any ]], ): \"\"\" Uses the underlying boto3 waiter functionality. Args: client: The AWS client on which to wait (e.g., 'client_wait', 'ec2', etc). waiter_name: The name of the waiter to instantiate. You may also use a custom waiter name, if you supply an accompanying waiter definition dict. aws_credentials: Credentials to use for authentication with AWS. waiter_definition: A valid custom waiter model, as a dict. Note that if you supply a custom definition, it is assumed that the provided 'waiter_name' is contained within the waiter definition dict. **waiter_kwargs: Arguments to pass to the `waiter.wait(...)` method. Will depend upon the specific waiter being called. Example: Run an ec2 waiter until instance_exists. ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.client_wait import client_waiter @flow def example_client_wait_flow(): aws_credentials = AwsCredentials( aws_access_key_id=\"acccess_key_id\", aws_secret_access_key=\"secret_access_key\" ) waiter = client_waiter( \"ec2\", \"instance_exists\", aws_credentials ) return waiter example_client_wait_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Waiting on %s job\" , client ) boto_client = aws_credentials . get_boto3_session () . client ( client ) if waiter_definition is not None : # Use user-provided waiter definition waiter_model = WaiterModel ( waiter_definition ) waiter = create_waiter_with_client ( waiter_name , waiter_model , boto_client ) elif waiter_name in boto_client . waiter_names : waiter = boto_client . get_waiter ( waiter_name ) else : raise ValueError ( f \"The waiter name, { waiter_name } , is not a valid boto waiter; \" f \"if using a custom waiter, you must provide a waiter definition\" ) await run_sync_in_worker_thread ( waiter . wait , ** waiter_kwargs )","title":"client_waiter()"},{"location":"credentials/","text":"prefect_aws.credentials Module handling AWS credentials AwsCredentials Block used to manage authentication with AWS. AWS authentication is handled via the boto3 module. Refer to the boto3 docs for more info about the possible credential configurations. Parameters: Name Type Description Default aws_access_key_id A specific AWS access key ID. required aws_secret_access_key A specific AWS secret access key. required aws_session_token The session key for your AWS account. This is only needed when you are using temporary credentials. required profile_name The profile to use when creating your session. required region_name The AWS Region where you want to create new connections. required Example Load stored AWS credentials: from prefect_aws import AwsCredentials aws_credentials_block = AwsCredentials . load ( \"BLOCK_NAME\" ) Source code in prefect_aws/credentials.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class AwsCredentials ( Block ): \"\"\" Block used to manage authentication with AWS. AWS authentication is handled via the `boto3` module. Refer to the [boto3 docs](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) for more info about the possible credential configurations. Args: aws_access_key_id: A specific AWS access key ID. aws_secret_access_key: A specific AWS secret access key. aws_session_token: The session key for your AWS account. This is only needed when you are using temporary credentials. profile_name: The profile to use when creating your session. region_name: The AWS Region where you want to create new connections. Example: Load stored AWS credentials: ```python from prefect_aws import AwsCredentials aws_credentials_block = AwsCredentials.load(\"BLOCK_NAME\") ``` \"\"\" # noqa E501 _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/1jbV4lceHOjGgunX15lUwT/db88e184d727f721575aeb054a37e277/aws.png?h=250\" # noqa _block_type_name = \"AWS Credentials\" aws_access_key_id : Optional [ str ] = None aws_secret_access_key : Optional [ SecretStr ] = None aws_session_token : Optional [ str ] = None profile_name : Optional [ str ] = None region_name : Optional [ str ] = None def get_boto3_session ( self ) -> boto3 . Session : \"\"\" Returns an authenticated boto3 session that can be used to create clients for AWS services Example: Create an S3 client from an authorized boto3 session: ```python aws_credentials = AwsCredentials( aws_access_key_id = \"access_key_id\", aws_secret_access_key = \"secret_access_key\" ) s3_client = aws_credentials.get_boto3_session().client(\"s3\") ``` \"\"\" if self . aws_secret_access_key : aws_secret_access_key = self . aws_secret_access_key . get_secret_value () else : aws_secret_access_key = None return boto3 . Session ( aws_access_key_id = self . aws_access_key_id , aws_secret_access_key = aws_secret_access_key , aws_session_token = self . aws_session_token , profile_name = self . profile_name , region_name = self . region_name , ) get_boto3_session Returns an authenticated boto3 session that can be used to create clients for AWS services Example Create an S3 client from an authorized boto3 session: aws_credentials = AwsCredentials ( aws_access_key_id = \"access_key_id\" , aws_secret_access_key = \"secret_access_key\" ) s3_client = aws_credentials . get_boto3_session () . client ( \"s3\" ) Source code in prefect_aws/credentials.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def get_boto3_session ( self ) -> boto3 . Session : \"\"\" Returns an authenticated boto3 session that can be used to create clients for AWS services Example: Create an S3 client from an authorized boto3 session: ```python aws_credentials = AwsCredentials( aws_access_key_id = \"access_key_id\", aws_secret_access_key = \"secret_access_key\" ) s3_client = aws_credentials.get_boto3_session().client(\"s3\") ``` \"\"\" if self . aws_secret_access_key : aws_secret_access_key = self . aws_secret_access_key . get_secret_value () else : aws_secret_access_key = None return boto3 . Session ( aws_access_key_id = self . aws_access_key_id , aws_secret_access_key = aws_secret_access_key , aws_session_token = self . aws_session_token , profile_name = self . profile_name , region_name = self . region_name , ) MinIOCredentials Block used to manage authentication with MinIO. Refer to the MinIO docs for more info about the possible credential configurations. Parameters: Name Type Description Default minio_root_user Admin or root user. required minio_root_password Admin or root password. required region_name Location of server, e.g. \"us-east-1\". required Example Load stored MinIO credentials: from prefect_aws import MinIOCredentials minio_credentials_block = MinIOCredentials . load ( \"BLOCK_NAME\" ) Source code in prefect_aws/credentials.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 class MinIOCredentials ( Block ): \"\"\" Block used to manage authentication with MinIO. Refer to the [MinIO docs](https://docs.min.io/docs/minio-server-configuration-guide.html) for more info about the possible credential configurations. Args: minio_root_user: Admin or root user. minio_root_password: Admin or root password. region_name: Location of server, e.g. \"us-east-1\". Example: Load stored MinIO credentials: ```python from prefect_aws import MinIOCredentials minio_credentials_block = MinIOCredentials.load(\"BLOCK_NAME\") ``` \"\"\" # noqa E501 _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/22vXcxsOrVeFrUwHfSoaeT/7607b876eb589a9028c8126e78f4c7b4/imageedit_7_2837870043.png?h=250\" # noqa _block_type_name = \"MinIO Credentials\" _description = ( \"Block used to manage authentication with MinIO. Refer to the MinIO \" \"docs: https://docs.min.io/docs/minio-server-configuration-guide.html \" \"for more info about the possible credential configurations.\" ) minio_root_user : str minio_root_password : SecretStr region_name : Optional [ str ] = None def get_boto3_session ( self ) -> boto3 . Session : \"\"\" Returns an authenticated boto3 session that can be used to create clients and perform object operations on MinIO server. Example: Create an S3 client from an authorized boto3 session ```python minio_credentials = MinIOCredentials( minio_root_user = \"minio_root_user\", minio_root_password = \"minio_root_password\" ) s3_client = minio_credentials.get_boto3_session().client( service=\"s3\", endpoint_url=\"http://localhost:9000\" ) ``` \"\"\" minio_root_password = ( self . minio_root_password . get_secret_value () if self . minio_root_password else None ) return boto3 . Session ( aws_access_key_id = self . minio_root_user , aws_secret_access_key = minio_root_password , region_name = self . region_name , ) get_boto3_session Returns an authenticated boto3 session that can be used to create clients and perform object operations on MinIO server. Example Create an S3 client from an authorized boto3 session minio_credentials = MinIOCredentials ( minio_root_user = \"minio_root_user\" , minio_root_password = \"minio_root_password\" ) s3_client = minio_credentials . get_boto3_session () . client ( service = \"s3\" , endpoint_url = \"http://localhost:9000\" ) Source code in prefect_aws/credentials.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def get_boto3_session ( self ) -> boto3 . Session : \"\"\" Returns an authenticated boto3 session that can be used to create clients and perform object operations on MinIO server. Example: Create an S3 client from an authorized boto3 session ```python minio_credentials = MinIOCredentials( minio_root_user = \"minio_root_user\", minio_root_password = \"minio_root_password\" ) s3_client = minio_credentials.get_boto3_session().client( service=\"s3\", endpoint_url=\"http://localhost:9000\" ) ``` \"\"\" minio_root_password = ( self . minio_root_password . get_secret_value () if self . minio_root_password else None ) return boto3 . Session ( aws_access_key_id = self . minio_root_user , aws_secret_access_key = minio_root_password , region_name = self . region_name , )","title":"Credentials"},{"location":"credentials/#prefect_aws.credentials","text":"Module handling AWS credentials","title":"credentials"},{"location":"credentials/#prefect_aws.credentials.AwsCredentials","text":"Block used to manage authentication with AWS. AWS authentication is handled via the boto3 module. Refer to the boto3 docs for more info about the possible credential configurations. Parameters: Name Type Description Default aws_access_key_id A specific AWS access key ID. required aws_secret_access_key A specific AWS secret access key. required aws_session_token The session key for your AWS account. This is only needed when you are using temporary credentials. required profile_name The profile to use when creating your session. required region_name The AWS Region where you want to create new connections. required Example Load stored AWS credentials: from prefect_aws import AwsCredentials aws_credentials_block = AwsCredentials . load ( \"BLOCK_NAME\" ) Source code in prefect_aws/credentials.py 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 class AwsCredentials ( Block ): \"\"\" Block used to manage authentication with AWS. AWS authentication is handled via the `boto3` module. Refer to the [boto3 docs](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html) for more info about the possible credential configurations. Args: aws_access_key_id: A specific AWS access key ID. aws_secret_access_key: A specific AWS secret access key. aws_session_token: The session key for your AWS account. This is only needed when you are using temporary credentials. profile_name: The profile to use when creating your session. region_name: The AWS Region where you want to create new connections. Example: Load stored AWS credentials: ```python from prefect_aws import AwsCredentials aws_credentials_block = AwsCredentials.load(\"BLOCK_NAME\") ``` \"\"\" # noqa E501 _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/1jbV4lceHOjGgunX15lUwT/db88e184d727f721575aeb054a37e277/aws.png?h=250\" # noqa _block_type_name = \"AWS Credentials\" aws_access_key_id : Optional [ str ] = None aws_secret_access_key : Optional [ SecretStr ] = None aws_session_token : Optional [ str ] = None profile_name : Optional [ str ] = None region_name : Optional [ str ] = None def get_boto3_session ( self ) -> boto3 . Session : \"\"\" Returns an authenticated boto3 session that can be used to create clients for AWS services Example: Create an S3 client from an authorized boto3 session: ```python aws_credentials = AwsCredentials( aws_access_key_id = \"access_key_id\", aws_secret_access_key = \"secret_access_key\" ) s3_client = aws_credentials.get_boto3_session().client(\"s3\") ``` \"\"\" if self . aws_secret_access_key : aws_secret_access_key = self . aws_secret_access_key . get_secret_value () else : aws_secret_access_key = None return boto3 . Session ( aws_access_key_id = self . aws_access_key_id , aws_secret_access_key = aws_secret_access_key , aws_session_token = self . aws_session_token , profile_name = self . profile_name , region_name = self . region_name , )","title":"AwsCredentials"},{"location":"credentials/#prefect_aws.credentials.AwsCredentials.get_boto3_session","text":"Returns an authenticated boto3 session that can be used to create clients for AWS services Example Create an S3 client from an authorized boto3 session: aws_credentials = AwsCredentials ( aws_access_key_id = \"access_key_id\" , aws_secret_access_key = \"secret_access_key\" ) s3_client = aws_credentials . get_boto3_session () . client ( \"s3\" ) Source code in prefect_aws/credentials.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 def get_boto3_session ( self ) -> boto3 . Session : \"\"\" Returns an authenticated boto3 session that can be used to create clients for AWS services Example: Create an S3 client from an authorized boto3 session: ```python aws_credentials = AwsCredentials( aws_access_key_id = \"access_key_id\", aws_secret_access_key = \"secret_access_key\" ) s3_client = aws_credentials.get_boto3_session().client(\"s3\") ``` \"\"\" if self . aws_secret_access_key : aws_secret_access_key = self . aws_secret_access_key . get_secret_value () else : aws_secret_access_key = None return boto3 . Session ( aws_access_key_id = self . aws_access_key_id , aws_secret_access_key = aws_secret_access_key , aws_session_token = self . aws_session_token , profile_name = self . profile_name , region_name = self . region_name , )","title":"get_boto3_session()"},{"location":"credentials/#prefect_aws.credentials.MinIOCredentials","text":"Block used to manage authentication with MinIO. Refer to the MinIO docs for more info about the possible credential configurations. Parameters: Name Type Description Default minio_root_user Admin or root user. required minio_root_password Admin or root password. required region_name Location of server, e.g. \"us-east-1\". required Example Load stored MinIO credentials: from prefect_aws import MinIOCredentials minio_credentials_block = MinIOCredentials . load ( \"BLOCK_NAME\" ) Source code in prefect_aws/credentials.py 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 class MinIOCredentials ( Block ): \"\"\" Block used to manage authentication with MinIO. Refer to the [MinIO docs](https://docs.min.io/docs/minio-server-configuration-guide.html) for more info about the possible credential configurations. Args: minio_root_user: Admin or root user. minio_root_password: Admin or root password. region_name: Location of server, e.g. \"us-east-1\". Example: Load stored MinIO credentials: ```python from prefect_aws import MinIOCredentials minio_credentials_block = MinIOCredentials.load(\"BLOCK_NAME\") ``` \"\"\" # noqa E501 _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/22vXcxsOrVeFrUwHfSoaeT/7607b876eb589a9028c8126e78f4c7b4/imageedit_7_2837870043.png?h=250\" # noqa _block_type_name = \"MinIO Credentials\" _description = ( \"Block used to manage authentication with MinIO. Refer to the MinIO \" \"docs: https://docs.min.io/docs/minio-server-configuration-guide.html \" \"for more info about the possible credential configurations.\" ) minio_root_user : str minio_root_password : SecretStr region_name : Optional [ str ] = None def get_boto3_session ( self ) -> boto3 . Session : \"\"\" Returns an authenticated boto3 session that can be used to create clients and perform object operations on MinIO server. Example: Create an S3 client from an authorized boto3 session ```python minio_credentials = MinIOCredentials( minio_root_user = \"minio_root_user\", minio_root_password = \"minio_root_password\" ) s3_client = minio_credentials.get_boto3_session().client( service=\"s3\", endpoint_url=\"http://localhost:9000\" ) ``` \"\"\" minio_root_password = ( self . minio_root_password . get_secret_value () if self . minio_root_password else None ) return boto3 . Session ( aws_access_key_id = self . minio_root_user , aws_secret_access_key = minio_root_password , region_name = self . region_name , )","title":"MinIOCredentials"},{"location":"credentials/#prefect_aws.credentials.MinIOCredentials.get_boto3_session","text":"Returns an authenticated boto3 session that can be used to create clients and perform object operations on MinIO server. Example Create an S3 client from an authorized boto3 session minio_credentials = MinIOCredentials ( minio_root_user = \"minio_root_user\" , minio_root_password = \"minio_root_password\" ) s3_client = minio_credentials . get_boto3_session () . client ( service = \"s3\" , endpoint_url = \"http://localhost:9000\" ) Source code in prefect_aws/credentials.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 def get_boto3_session ( self ) -> boto3 . Session : \"\"\" Returns an authenticated boto3 session that can be used to create clients and perform object operations on MinIO server. Example: Create an S3 client from an authorized boto3 session ```python minio_credentials = MinIOCredentials( minio_root_user = \"minio_root_user\", minio_root_password = \"minio_root_password\" ) s3_client = minio_credentials.get_boto3_session().client( service=\"s3\", endpoint_url=\"http://localhost:9000\" ) ``` \"\"\" minio_root_password = ( self . minio_root_password . get_secret_value () if self . minio_root_password else None ) return boto3 . Session ( aws_access_key_id = self . minio_root_user , aws_secret_access_key = minio_root_password , region_name = self . region_name , )","title":"get_boto3_session()"},{"location":"ecs/","text":"prefect_aws.ecs Integrations with the Amazon Elastic Container Service. Note this module is experimental. The intefaces within may change without notice. Examples: Run a task using ECS Fargate ECSTask ( command = [ \"echo\" , \"hello world\" ]) . run () Run a task using ECS Fargate with a spot container instance ECSTask ( command = [ \"echo\" , \"hello world\" ], launch_type = \"FARGATE_SPOT\" ) . run () Run a task using ECS with an EC2 container instance ECSTask ( command = [ \"echo\" , \"hello world\" ], launch_type = \"EC2\" ) . run () Run a task on a specific VPC using ECS Fargate ECSTask ( command = [ \"echo\" , \"hello world\" ], vpc_id = \"vpc-01abcdf123456789a\" ) . run () Run a task and stream the container's output to the local terminal. Note an execution role must be provided with permissions: logs:CreateLogStream, logs:CreateLogGroup, and logs:PutLogEvents. ECSTask ( command = [ \"echo\" , \"hello world\" ], stream_output = True , execution_role_arn = \"...\" ) Run a task using an existing task definition as a base ECSTask ( command = [ \"echo\" , \"hello world\" ], task_definition_arn = \"arn:aws:ecs:...\" ) Run a task with a specific image ECSTask ( command = [ \"echo\" , \"hello world\" ], image = \"alpine:latest\" ) Run a task with custom memory and CPU requirements ECSTask ( command = [ \"echo\" , \"hello world\" ], memory = 4096 , cpu = 2048 ) Run a task with custom environment variables ECSTask ( command = [ \"echo\" , \"hello $PLANET\" ], env = { \"PLANET\" : \"earth\" }) Run a task in a specific ECS cluster ECSTask ( command = [ \"echo\" , \"hello world\" ], cluster = \"my-cluster-name\" ) ECSTask Run a command as an ECS task. Note this block is experimental. The interface may change without notice. Source code in prefect_aws/ecs.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 class ECSTask ( Infrastructure ): \"\"\" <span class=\"badge-api experimental\"/> Run a command as an ECS task. Note this block is experimental. The interface may change without notice. \"\"\" _block_type_slug = \"ecs-task\" _block_type_name = \"ECS Task\" _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/1jbV4lceHOjGgunX15lUwT/db88e184d727f721575aeb054a37e277/aws.png?h=250\" # noqa _description = \"Run a command as an ECS task. Note this block is experimental. The interface may change without notice.\" # noqa type : Literal [ \"ecs-task\" ] = Field ( \"ecs-task\" , description = \"The slug for this task type.\" ) aws_credentials : AwsCredentials = Field ( title = \"AWS Credentials\" , default_factory = AwsCredentials , description = \"The AWS credentials to use to connect to ECS.\" , ) # Task definition settings task_definition_arn : Optional [ str ] = Field ( default = None , description = ( \"An identifier for an existing task definition to use. If fields are set \" \"on the `ECSTask` that conflict with the task definition, a new copy \" \"will be registered with the required values. \" \"Cannot be used with `task_definition`. If not provided, Prefect will \" \"generate and register a minimal task definition.\" ), ) task_definition : Optional [ dict ] = Field ( default = None , description = ( \"An ECS task definition to use. Prefect may set defaults or override \" \"fields on this task definition to match other `ECSTask` fields. \" \"Cannot be used with `task_definition_arn`. If not provided, Prefect will \" \"generate and register a minimal task definition.\" ), ) image : Optional [ str ] = Field ( default_factory = get_prefect_image_name , description = ( \"The image to use for the Prefect container in the task. If this value is \" \"not null, it will override the value in the task definition. This value \" \"defaults to a Prefect base image matching your local versions.\" ), ) auto_deregister_task_definition : bool = Field ( default = True , description = ( \"If set, any task definitions that are created by this block will be \" \"deregistered. Existing task definitions linked by ARN will never be \" \"deregistered. Deregistering a task definition does not remove it from \" \"your AWS account, instead it will be marked as INACTIVE.\" ), ) # Mixed task definition / run settings cpu : int = Field ( title = \"CPU\" , default = None , description = ( \"The amount of CPU to provide to the ECS task. Valid amounts are \" \"specified in the AWS documentation. If not provided, a default value of \" f \" { ECS_DEFAULT_CPU } will be used unless present on the task definition.\" ), ) memory : int = Field ( default = None , description = ( \"The amount of memory to provide to the ECS task. Valid amounts are \" \"specified in the AWS documentation. If not provided, a default value of \" f \" { ECS_DEFAULT_MEMORY } will be used unless present on the task definition.\" ), ) execution_role_arn : str = Field ( title = \"Execution Role ARN\" , default = None , description = ( \"An execution role to use for the task. This controls the permissions of \" \"the task when it is launching. If this value is not null, it will \" \"override the value in the task definition. An execution role must be \" \"provided to capture logs from the container.\" ), ) configure_cloudwatch_logs : bool = Field ( default = None , description = ( \"If `True`, the Prefect container will be configured to send its output \" \"to the AWS CloudWatch logs service. This functionality requires an \" \"execution role with logs:CreateLogStream, logs:CreateLogGroup, and \" \"logs:PutLogEvents permissions. The default for this field is `False` \" \"unless `stream_output` is set. \" ), ) stream_output : bool = Field ( default = None , description = ( \"If `True`, logs will be streamed from the Prefect container to the local \" \"console. Unless you have configured AWS CloudWatch logs manually on your \" \"task definition, this requires the same prerequisites outlined in \" \"`configure_cloudwatch_logs`.\" ), ) # Task run settings launch_type : Optional [ Literal [ \"FARGATE\" , \"EC2\" , \"EXTERNAL\" , \"FARGATE_SPOT\" ] ] = Field ( default = \"FARGATE\" , description = ( \"The type of ECS task run infrastructure that should be used. Note that \" \"'FARGATE_SPOT' is not a formal ECS launch type, but we will configure the \" \"proper capacity provider stategy if set here.\" ), ) vpc_id : Optional [ str ] = Field ( title = \"VPC ID\" , default = None , description = ( \"The AWS VPC to link the task run to. This is only applicable when using \" \"the 'awsvpc' network mode for your task. FARGATE tasks require this \" \"network mode, but for EC2 tasks the default network mode is 'bridge'. \" \"If using the 'awsvpc' network mode and this field is null, your default \" \"VPC will be used. If no default VPC can be found, the task run will fail.\" ), ) cluster : Optional [ str ] = Field ( default = None , description = ( \"The ECS cluster to run the task in. The ARN or name may be provided. If \" \"not provided, the default cluster will be used.\" ), ) env : Dict [ str , Optional [ str ]] = Field ( title = \"Environment Variables\" , default_factory = dict , description = ( \"Environment variables to provide to the task run. These variables are set \" \"on the Prefect container at task runtime. These will not be set on the \" \"task definition.\" ), ) task_role_arn : str = Field ( title = \"Task Role ARN\" , default = None , description = ( \"A role to attach to the task run. This controls the permissions of the \" \"task while it is running.\" ), ) # Execution settings task_start_timeout_seconds : int = Field ( default = 30 , description = ( \"The amount of time to watch for the start of the ECS task \" \"before marking it as failed. The task must enter a RUNNING state to be \" \"considered started.\" ), ) task_watch_poll_interval : float = Field ( default = 5.0 , description = ( \"The amount of time to wait between AWS API calls while monitoring the \" \"state of an ECS task.\" ), ) @root_validator ( pre = True ) def set_default_configure_cloudwatch_logs ( cls , values : dict ) -> dict : \"\"\" Streaming output generally requires CloudWatch logs to be configured. To avoid entangled arguments in the simple case, `configure_cloudwatch_logs` defaults to matching the value of `stream_output`. \"\"\" configure_cloudwatch_logs = values . get ( \"configure_cloudwatch_logs\" ) if configure_cloudwatch_logs is None : values [ \"configure_cloudwatch_logs\" ] = values . get ( \"stream_output\" ) return values @root_validator def configure_cloudwatch_logs_requires_execution_role_arn ( cls , values : dict ) -> dict : \"\"\" Enforces that an execution role arn is provided (or could be provided by a runtime task definition) when configuring logging. \"\"\" if ( values . get ( \"configure_cloudwatch_logs\" ) and not values . get ( \"execution_role_arn\" ) # Do not raise if they've linked to another task definition or provided # it without using our shortcuts and not values . get ( \"task_definition_arn\" ) and not ( values . get ( \"task_definition\" ) or {}) . get ( \"executionRoleArn\" ) ): raise ValueError ( \"An `execution_role_arn` must be provided to use \" \"`configure_cloudwatch_logs` or `stream_logs`.\" ) return values @root_validator def image_is_required ( cls , values : dict ) -> dict : \"\"\" Enforces that an image is available if the user sets it to `None`. \"\"\" if ( not values . get ( \"image\" ) and not values . get ( \"task_definition_arn\" ) # Check for an image in the task definition; whew! and not ( get_prefect_container ( ( values . get ( \"task_definition\" ) or {}) . get ( \"containerDefinitions\" , [] ) ) or {} ) . get ( \"image\" ) ): raise ValueError ( \"A value for the `image` field must be provided unless already \" \"present for the Prefect container definition a given task definition.\" ) return values @sync_compatible async def run ( self , task_status : Optional [ TaskStatus ] = None ) -> ECSTaskResult : \"\"\" Run the configured task on ECS. \"\"\" boto_session , ecs_client = await run_sync_in_worker_thread ( self . _get_session_and_client ) ( task_arn , cluster_arn , task_definition , is_new_task_definition , ) = await run_sync_in_worker_thread ( self . _create_task_and_wait_for_start , boto_session , ecs_client ) # Display a nice message indicating the command and image command = self . command or get_prefect_container ( task_definition [ \"containerDefinitions\" ] ) . get ( \"command\" , []) self . logger . info ( f \" { self . _log_prefix } : Running command { ' ' . join ( command ) !r} \" f \"in container { PREFECT_ECS_CONTAINER_NAME !r} ( { self . image } )...\" ) if task_status : task_status . started ( task_arn ) status_code = await run_sync_in_worker_thread ( self . _watch_task_and_get_exit_code , task_arn , cluster_arn , task_definition , is_new_task_definition and self . auto_deregister_task_definition , boto_session , ecs_client , ) return ECSTaskResult ( identifier = task_arn , # If the container does not start the exit code can be null but we must # still report a status code. We use a -1 to indicate a special code. status_code = status_code or - 1 , ) @property def _log_prefix ( self ) -> str : \"\"\" Internal property for generating a prefix for logs where `name` may be null \"\"\" if self . name is not None : return f \"ECSTask { self . name !r} \" else : return \"ECSTask\" def _get_session_and_client ( self ) -> Tuple [ boto3 . Session , _ECSClient ]: \"\"\" Retrieve a boto3 session and ECS client \"\"\" boto_session = self . aws_credentials . get_boto3_session () ecs_client = boto_session . client ( \"ecs\" ) return boto_session , ecs_client def _create_task_and_wait_for_start ( self , boto_session : boto3 . Session , ecs_client : _ECSClient ) -> Tuple [ str , str , dict , bool ]: \"\"\" Register the task definition, create the task run, and wait for it to start. Returns a tuple of - The task ARN - The task's cluster ARN - The task definition - A bool indicating if the task definition is newly registered \"\"\" new_task_definition_registered = False requested_task_definition = ( self . _retrieve_task_definition ( ecs_client , self . task_definition_arn ) if self . task_definition_arn else self . task_definition ) or {} task_definition_arn = requested_task_definition . get ( \"taskDefinitionArn\" , None ) task_definition = self . _prepare_task_definition ( requested_task_definition , region = ecs_client . meta . region_name ) # We must register the task definition if the arn is null or changes were made if task_definition != requested_task_definition or not task_definition_arn : self . logger . info ( f \" { self . _log_prefix } : Registering task definition...\" ) self . logger . debug ( \"Task definition payload \\n \" + yaml . dump ( task_definition )) task_definition_arn = self . _register_task_definition ( ecs_client , task_definition ) new_task_definition_registered = True if task_definition . get ( \"networkMode\" ) == \"awsvpc\" : network_config = self . _load_vpc_network_config ( self . vpc_id , boto_session ) else : network_config = None task_run = self . _prepare_task_run ( network_config = network_config , task_definition_arn = task_definition_arn , ) self . logger . info ( f \" { self . _log_prefix } : Creating task run...\" ) self . logger . debug ( \"Task run payload \\n \" + yaml . dump ( task_run )) try : task = self . _run_task ( ecs_client , task_run ) task_arn = task [ \"taskArn\" ] cluster_arn = task [ \"clusterArn\" ] except Exception as exc : self . _report_task_run_creation_failure ( task_run , exc ) # Raises an exception if the task does not start self . logger . info ( f \" { self . _log_prefix } : Waiting for task run to start...\" ) self . _wait_for_task_start ( task_arn , cluster_arn , ecs_client , timeout = self . task_start_timeout_seconds ) return task_arn , cluster_arn , task_definition , new_task_definition_registered def _watch_task_and_get_exit_code ( self , task_arn : str , cluster_arn : str , task_definition : dict , deregister_task_definition : bool , boto_session : boto3 . Session , ecs_client : _ECSClient , ) -> Optional [ int ]: \"\"\" Wait for the task run to complete and retrieve the exit code of the Prefect container. \"\"\" # Wait for completion and stream logs task = self . _wait_for_task_finish ( task_arn , cluster_arn , task_definition , ecs_client , boto_session ) if deregister_task_definition : ecs_client . deregister_task_definition ( taskDefinition = task [ \"taskDefinitionArn\" ] ) # Check the status code of the Prefect container prefect_container = get_prefect_container ( task [ \"containers\" ]) assert ( prefect_container is not None ), f \"'prefect' container missing from task: { task } \" status_code = prefect_container . get ( \"exitCode\" ) self . _report_container_status_code ( PREFECT_ECS_CONTAINER_NAME , status_code ) return status_code def preview ( self ) -> str : \"\"\" Generate a preview of the task definition and task run that will be sent to AWS. \"\"\" preview = \"\" task_definition_arn = self . task_definition_arn or \"<registered at runtime>\" if self . task_definition or not self . task_definition_arn : task_definition = self . _prepare_task_definition ( self . task_definition or {}, region = self . aws_credentials . region_name or \"<loaded from client at runtime>\" , ) preview += \"--- \\n # Task definition \\n \" preview += yaml . dump ( task_definition ) preview += \" \\n \" else : task_definition = None if task_definition and task_definition . get ( \"networkMode\" ) == \"awsvpc\" : vpc = \"the default VPC\" if not self . vpc_id else self . vpc_id network_config = { \"awsvpcConfiguration\" : f \"<loaded from { vpc } at runtime>\" } else : network_config = None task_run = self . _prepare_task_run ( network_config , task_definition_arn ) preview += \"--- \\n # Task run request \\n \" preview += yaml . dump ( task_run ) return preview def _report_container_status_code ( self , name : str , status_code : Optional [ int ] ) -> None : \"\"\" Display a log for the given container status code. \"\"\" if status_code is None : self . logger . error ( f \" { self . _log_prefix } : Task exited without reporting an exit status \" f \"for container { name !r} .\" ) elif status_code == 0 : self . logger . info ( f \" { self . _log_prefix } : Container { name !r} exited successfully.\" ) else : self . logger . warning ( f \" { self . _log_prefix } : Container { name !r} exited with non-zero exit \" f \"code { status_code } .\" ) def _report_task_run_creation_failure ( self , task_run : dict , exc : Exception ) -> None : \"\"\" Wrap common AWS task run creation failures with nicer user-facing messages. \"\"\" # AWS generates exception types at runtime so they must be captured a bit # differently than normal. if \"ClusterNotFoundException\" in str ( exc ): cluster = task_run . get ( \"cluster\" , \"default\" ) raise RuntimeError ( f \"Failed to run ECS task, cluster { cluster !r} not found. \" \"Confirm that the cluster is configured in your region.\" ) from exc elif \"No Container Instances\" in str ( exc ) and self . launch_type == \"EC2\" : cluster = task_run . get ( \"cluster\" , \"default\" ) raise RuntimeError ( f \"Failed to run ECS task, cluster { cluster !r} does not appear to \" \"have any container instances associated with it. Confirm that you \" \"have EC2 container instances available.\" ) from exc elif ( \"failed to validate logger args\" in str ( exc ) and \"AccessDeniedException\" in str ( exc ) and self . configure_cloudwatch_logs ): raise RuntimeError ( f \"Failed to run ECS task, the attached execution role does not appear \" \"to have sufficient permissions. Ensure that the execution role \" f \" { self . execution_role !r} has permissions logs:CreateLogStream, \" \"logs:CreateLogGroup, and logs:PutLogEvents.\" ) else : raise def _watch_task_run ( self , task_arn : str , cluster_arn : str , ecs_client : _ECSClient , current_status : str = \"UNKNOWN\" , until_status : str = None , timeout : int = None , ) -> Generator [ None , None , dict ]: \"\"\" Watches an ECS task run by querying every `poll_interval` seconds. After each query, the retrieved task is yielded. This function returns when the task run reaches a STOPPED status or the provided `until_status`. Emits a log each time the status changes. \"\"\" last_status = status = current_status t0 = time . time () while status != until_status : task = ecs_client . describe_tasks ( tasks = [ task_arn ], cluster = cluster_arn )[ \"tasks\" ][ 0 ] status = task [ \"lastStatus\" ] if status != last_status : self . logger . info ( f \" { self . _log_prefix } : Status is { status } .\" ) yield task # No point in continuing if the status is final if status == \"STOPPED\" : break last_status = status elapsed_time = time . time () - t0 if timeout is not None and elapsed_time > timeout : raise RuntimeError ( f \"Timed out after { elapsed_time } s while watching task for status \" \"{until_status or 'STOPPED'}\" ) time . sleep ( self . task_watch_poll_interval ) def _wait_for_task_start ( self , task_arn : str , cluster_arn : str , ecs_client : _ECSClient , timeout : int ) -> dict : \"\"\" Waits for an ECS task run to reach a RUNNING status. If a STOPPED status is reached instead, an exception is raised indicating the reason that the task run did not start. \"\"\" for task in self . _watch_task_run ( task_arn , cluster_arn , ecs_client , until_status = \"RUNNING\" , timeout = timeout ): # TODO: It is possible that the task has passed _through_ a RUNNING # status during the polling interval. In this case, there is not an # exception to raise. if task [ \"lastStatus\" ] == \"STOPPED\" : code = task . get ( \"stopCode\" ) reason = task . get ( \"stoppedReason\" ) # Generate a dynamic exception type from the AWS name raise type ( code , ( RuntimeError ,), {})( reason ) return task def _wait_for_task_finish ( self , task_arn : str , cluster_arn : str , task_definition : dict , ecs_client : _ECSClient , boto_session : boto3 . Session , ): \"\"\" Watch an ECS task until it reaches a STOPPED status. If configured, logs from the Prefect container are streamed to stderr. Returns a description of the task on completion. \"\"\" can_stream_output = False if self . stream_output : container_def = get_prefect_container ( task_definition [ \"containerDefinitions\" ] ) if not container_def : self . logger . warning ( f \" { self . _log_prefix } : Prefect container definition not found in \" \"task definition. Output cannot be streamed.\" ) elif not container_def . get ( \"logConfiguration\" ): self . logger . warning ( f \" { self . _log_prefix } : Logging configuration not found on task. \" \"Output cannot be streamed.\" ) elif not container_def [ \"logConfiguration\" ] . get ( \"logDriver\" ) == \"awslogs\" : self . logger . warning ( f \" { self . _log_prefix } : Logging configuration uses unsupported \" \" driver {container_def['logConfiguration'].get('logDriver')!r}. \" \"Output cannot be streamed.\" ) else : # Prepare to stream the output log_config = container_def [ \"logConfiguration\" ][ \"options\" ] logs_client = boto_session . client ( \"logs\" ) can_stream_output = True # Track the last log timestamp to prevent double display last_log_timestamp : Optional [ int ] = None # Determine the name of the stream as \"prefix/family/run\" stream_name = \"/\" . join ( [ log_config [ \"awslogs-stream-prefix\" ], task_definition [ \"family\" ], task_arn . rsplit ( \"/\" )[ - 1 ], ] ) self . logger . info ( f \" { self . _log_prefix } : Streaming output from container \" f \" { PREFECT_ECS_CONTAINER_NAME !r} ...\" ) for task in self . _watch_task_run ( task_arn , cluster_arn , ecs_client , current_status = \"RUNNING\" ): if self . stream_output and can_stream_output : # On each poll for task run status, also retrieve available logs last_log_timestamp = self . _stream_available_logs ( logs_client , log_group = log_config [ \"awslogs-group\" ], log_stream = stream_name , last_log_timestamp = last_log_timestamp , ) return task def _stream_available_logs ( self , logs_client : Any , log_group : str , log_stream : str , last_log_timestamp : Optional [ int ] = None , ) -> Optional [ int ]: \"\"\" Stream logs from the given log group and stream since the last log timestamp. Will continue on paginated responses until all logs are returned. Returns the last log timestamp which can be used to call this method in the future. \"\"\" last_log_stream_token = \"NO-TOKEN\" next_log_stream_token = None # AWS will return the same token that we send once the end of the paginated # response is reached while last_log_stream_token != next_log_stream_token : last_log_stream_token = next_log_stream_token request = { \"logGroupName\" : log_group , \"logStreamName\" : log_stream , } if last_log_stream_token is not None : request [ \"nextToken\" ] = last_log_stream_token if last_log_timestamp is not None : # Bump the timestamp by one ms to avoid retrieving the last log again request [ \"startTime\" ] = last_log_timestamp + 1 response = logs_client . get_log_events ( ** request ) log_events = response [ \"events\" ] for log_event in log_events : # TODO: This doesn't forward to the local logger, which can be # bad for customizing handling and understanding where the # log is coming from, but it avoid nesting logger information # when the content is output from a Prefect logger on the # running infrastructure print ( log_event [ \"message\" ], file = sys . stderr ) if ( last_log_timestamp is None or log_event [ \"timestamp\" ] > last_log_timestamp ): last_log_timestamp = log_event [ \"timestamp\" ] next_log_stream_token = response . get ( \"nextForwardToken\" ) return last_log_timestamp def _retrieve_task_definition ( self , ecs_client : _ECSClient , task_definition_arn : str ): \"\"\" Retrieve an existing task definition from AWS. \"\"\" self . logger . info ( f \" { self . _log_prefix } : \" f \"Retrieving task definition { task_definition_arn !r} ...\" ) response = ecs_client . describe_task_definition ( taskDefinition = task_definition_arn ) return response [ \"taskDefinition\" ] def _register_task_definition ( self , ecs_client : _ECSClient , task_definition : dict ) -> str : \"\"\" Register a new task definition with AWS. \"\"\" # TODO: Consider including a global cache for this task definition since # registration of task definitions is frequently rate limited task_definition_request = copy . deepcopy ( task_definition ) # We need to remove some fields here if copying an existing task definition if self . task_definition_arn : task_definition_request . pop ( \"compatibilities\" , None ) task_definition_request . pop ( \"taskDefinitionArn\" , None ) task_definition_request . pop ( \"revision\" , None ) task_definition_request . pop ( \"status\" , None ) response = ecs_client . register_task_definition ( ** task_definition_request ) return response [ \"taskDefinition\" ][ \"taskDefinitionArn\" ] def _prepare_task_definition ( self , task_definition : dict , region : str ) -> dict : \"\"\" Prepare a task definition by inferring any defaults and merging overrides. \"\"\" task_definition = copy . deepcopy ( task_definition ) # Configure the Prefect runtime container task_definition . setdefault ( \"containerDefinitions\" , []) container = get_prefect_container ( task_definition [ \"containerDefinitions\" ]) if container is None : container = { \"name\" : PREFECT_ECS_CONTAINER_NAME } task_definition [ \"containerDefinitions\" ] . append ( container ) if self . image : container [ \"image\" ] = self . image # Remove any keys that have been explicitly \"unset\" unset_keys = { key for key , value in self . env . items () if value is None } for item in tuple ( container . get ( \"environment\" , [])): if item [ \"name\" ] in unset_keys : container [ \"environment\" ] . remove ( item ) if self . configure_cloudwatch_logs : container [ \"logConfiguration\" ] = { \"logDriver\" : \"awslogs\" , \"options\" : { \"awslogs-create-group\" : \"true\" , \"awslogs-group\" : \"prefect\" , \"awslogs-region\" : region , \"awslogs-stream-prefix\" : self . name or \"prefect\" , }, } task_definition . setdefault ( \"family\" , \"prefect\" ) # CPU and memory are required in some cases, retrieve the value to use cpu = self . cpu or task_definition . get ( \"cpu\" ) or ECS_DEFAULT_CPU memory = self . memory or task_definition . get ( \"memory\" ) or ECS_DEFAULT_MEMORY if self . launch_type == \"FARGATE\" or self . launch_type == \"FARGATE_SPOT\" : # Task level memory and cpu are required when using fargate task_definition [ \"cpu\" ] = str ( cpu ) task_definition [ \"memory\" ] = str ( memory ) # The FARGATE compatibility is required if it will be used as as launch type requires_compatibilities = task_definition . setdefault ( \"requiresCompatibilities\" , [] ) if \"FARGATE\" not in requires_compatibilities : task_definition [ \"requiresCompatibilities\" ] . append ( \"FARGATE\" ) # Only the 'awsvpc' network mode is supported when using FARGATE # However, we will not enforce that here if the user has set it network_mode = task_definition . setdefault ( \"networkMode\" , \"awsvpc\" ) if network_mode != \"awsvpc\" : warnings . warn ( f \"Found network mode { network_mode !r} which is not compatible with \" f \"launch type { self . launch_type !r} . Use either the 'EC2' launch \" \"type or the 'awsvpc' network mode.\" ) elif self . launch_type == \"EC2\" : # Container level memory and cpu are required when using ec2 container . setdefault ( \"cpu\" , int ( cpu )) container . setdefault ( \"memory\" , int ( memory )) if self . execution_role_arn and not self . task_definition_arn : task_definition [ \"executionRoleArn\" ] = self . execution_role_arn if self . configure_cloudwatch_logs and not task_definition . get ( \"executionRoleArn\" ): raise ValueError ( \"An execution role arn must be set on the task definition to use \" \"`configure_cloudwatch_logs` or `stream_logs` but no execution role \" \"was found on the task definition.\" ) return task_definition def _prepare_task_run_overrides ( self ) -> dict : \"\"\" Prepare the 'overrides' payload for a task run request. \"\"\" overrides = { \"containerOverrides\" : [ { \"name\" : PREFECT_ECS_CONTAINER_NAME , \"environment\" : [ { \"name\" : key , \"value\" : value } for key , value in { ** self . _base_environment (), ** self . env , } . items () if value is not None ], } ], } prefect_container_overrides = overrides [ \"containerOverrides\" ][ 0 ] if self . command : prefect_container_overrides [ \"command\" ] = self . command if self . execution_role_arn : overrides [ \"executionRoleArn\" ] = self . execution_role_arn if self . task_role_arn : overrides [ \"taskRoleArn\" ] = self . task_role_arn if self . memory : overrides [ \"memory\" ] = str ( self . memory ) prefect_container_overrides . setdefault ( \"memory\" , self . memory ) if self . cpu : overrides [ \"cpu\" ] = str ( self . cpu ) prefect_container_overrides . setdefault ( \"cpu\" , self . cpu ) return overrides def _load_vpc_network_config ( self , vpc_id : Optional [ str ], boto_session : boto3 . Session ) -> dict : \"\"\" Load settings from a specific VPC or the default VPC and generate a task run request's network configuration. \"\"\" ec2_client = boto_session . client ( \"ec2\" ) vpc_message = \"the default VPC\" if not vpc_id else f \"VPC with ID { vpc_id } \" if not vpc_id : # Retrieve the default VPC describe = { \"Filters\" : [{ \"Name\" : \"isDefault\" , \"Values\" : [ \"true\" ]}]} else : describe = { \"VpcIds\" : [ vpc_id ]} vpcs = ec2_client . describe_vpcs ( ** describe )[ \"Vpcs\" ] if not vpcs : help_message = ( \"Pass an explicit `vpc_id` or configure a default VPC.\" if not vpc_id else \"Check that the VPC exists in the current region.\" ) raise ValueError ( f \"Failed to find { vpc_message } . \" \"Network configuration cannot be inferred. \" + help_message ) vpc_id = vpcs [ 0 ][ \"VpcId\" ] subnets = ec2_client . describe_subnets ( Filters = [{ \"Name\" : \"vpc-id\" , \"Values\" : [ vpc_id ]}] )[ \"Subnets\" ] if not subnets : raise ValueError ( f \"Failed to find subnets for { vpc_message } . \" \"Network configuration cannot be inferred.\" ) return { \"awsvpcConfiguration\" : { \"subnets\" : [ s [ \"SubnetId\" ] for s in subnets ], \"assignPublicIp\" : \"ENABLED\" , } } def _prepare_task_run ( self , network_config : Optional [ dict ], task_definition_arn : str , ) -> dict : \"\"\" Prepare a task run request payload. \"\"\" task_run = { \"overrides\" : self . _prepare_task_run_overrides (), \"tags\" : [ { \"key\" : key , \"value\" : value } for key , value in self . labels . items () ], \"taskDefinition\" : task_definition_arn , } if self . cluster : task_run [ \"cluster\" ] = self . cluster if self . launch_type : if self . launch_type == \"FARGATE_SPOT\" : task_run [ \"capacityProviderStrategy\" ] = [ { \"capacityProvider\" : \"FARGATE_SPOT\" , \"weight\" : 1 } ] else : task_run [ \"launchType\" ] = self . launch_type if network_config : task_run [ \"networkConfiguration\" ] = network_config return task_run def _run_task ( self , ecs_client : _ECSClient , task_run : dict ): \"\"\" Run the task using the ECS client. This is isolated as a separate method for testing purposes. \"\"\" return ecs_client . run_task ( ** task_run )[ \"tasks\" ][ 0 ] configure_cloudwatch_logs_requires_execution_role_arn Enforces that an execution role arn is provided (or could be provided by a runtime task definition) when configuring logging. Source code in prefect_aws/ecs.py 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 @root_validator def configure_cloudwatch_logs_requires_execution_role_arn ( cls , values : dict ) -> dict : \"\"\" Enforces that an execution role arn is provided (or could be provided by a runtime task definition) when configuring logging. \"\"\" if ( values . get ( \"configure_cloudwatch_logs\" ) and not values . get ( \"execution_role_arn\" ) # Do not raise if they've linked to another task definition or provided # it without using our shortcuts and not values . get ( \"task_definition_arn\" ) and not ( values . get ( \"task_definition\" ) or {}) . get ( \"executionRoleArn\" ) ): raise ValueError ( \"An `execution_role_arn` must be provided to use \" \"`configure_cloudwatch_logs` or `stream_logs`.\" ) return values image_is_required Enforces that an image is available if the user sets it to None . Source code in prefect_aws/ecs.py 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 @root_validator def image_is_required ( cls , values : dict ) -> dict : \"\"\" Enforces that an image is available if the user sets it to `None`. \"\"\" if ( not values . get ( \"image\" ) and not values . get ( \"task_definition_arn\" ) # Check for an image in the task definition; whew! and not ( get_prefect_container ( ( values . get ( \"task_definition\" ) or {}) . get ( \"containerDefinitions\" , [] ) ) or {} ) . get ( \"image\" ) ): raise ValueError ( \"A value for the `image` field must be provided unless already \" \"present for the Prefect container definition a given task definition.\" ) return values preview Generate a preview of the task definition and task run that will be sent to AWS. Source code in prefect_aws/ecs.py 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 def preview ( self ) -> str : \"\"\" Generate a preview of the task definition and task run that will be sent to AWS. \"\"\" preview = \"\" task_definition_arn = self . task_definition_arn or \"<registered at runtime>\" if self . task_definition or not self . task_definition_arn : task_definition = self . _prepare_task_definition ( self . task_definition or {}, region = self . aws_credentials . region_name or \"<loaded from client at runtime>\" , ) preview += \"--- \\n # Task definition \\n \" preview += yaml . dump ( task_definition ) preview += \" \\n \" else : task_definition = None if task_definition and task_definition . get ( \"networkMode\" ) == \"awsvpc\" : vpc = \"the default VPC\" if not self . vpc_id else self . vpc_id network_config = { \"awsvpcConfiguration\" : f \"<loaded from { vpc } at runtime>\" } else : network_config = None task_run = self . _prepare_task_run ( network_config , task_definition_arn ) preview += \"--- \\n # Task run request \\n \" preview += yaml . dump ( task_run ) return preview run async Run the configured task on ECS. Source code in prefect_aws/ecs.py 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 @sync_compatible async def run ( self , task_status : Optional [ TaskStatus ] = None ) -> ECSTaskResult : \"\"\" Run the configured task on ECS. \"\"\" boto_session , ecs_client = await run_sync_in_worker_thread ( self . _get_session_and_client ) ( task_arn , cluster_arn , task_definition , is_new_task_definition , ) = await run_sync_in_worker_thread ( self . _create_task_and_wait_for_start , boto_session , ecs_client ) # Display a nice message indicating the command and image command = self . command or get_prefect_container ( task_definition [ \"containerDefinitions\" ] ) . get ( \"command\" , []) self . logger . info ( f \" { self . _log_prefix } : Running command { ' ' . join ( command ) !r} \" f \"in container { PREFECT_ECS_CONTAINER_NAME !r} ( { self . image } )...\" ) if task_status : task_status . started ( task_arn ) status_code = await run_sync_in_worker_thread ( self . _watch_task_and_get_exit_code , task_arn , cluster_arn , task_definition , is_new_task_definition and self . auto_deregister_task_definition , boto_session , ecs_client , ) return ECSTaskResult ( identifier = task_arn , # If the container does not start the exit code can be null but we must # still report a status code. We use a -1 to indicate a special code. status_code = status_code or - 1 , ) set_default_configure_cloudwatch_logs Streaming output generally requires CloudWatch logs to be configured. To avoid entangled arguments in the simple case, configure_cloudwatch_logs defaults to matching the value of stream_output . Source code in prefect_aws/ecs.py 289 290 291 292 293 294 295 296 297 298 299 300 @root_validator ( pre = True ) def set_default_configure_cloudwatch_logs ( cls , values : dict ) -> dict : \"\"\" Streaming output generally requires CloudWatch logs to be configured. To avoid entangled arguments in the simple case, `configure_cloudwatch_logs` defaults to matching the value of `stream_output`. \"\"\" configure_cloudwatch_logs = values . get ( \"configure_cloudwatch_logs\" ) if configure_cloudwatch_logs is None : values [ \"configure_cloudwatch_logs\" ] = values . get ( \"stream_output\" ) return values ECSTaskResult The result of a run of an ECS task Source code in prefect_aws/ecs.py 87 88 class ECSTaskResult ( InfrastructureResult ): \"\"\"The result of a run of an ECS task\"\"\" get_container Extract the a container from a list of containers or container definitions If not found, None is returned. Source code in prefect_aws/ecs.py 104 105 106 107 108 109 110 111 112 def get_container ( containers : List [ dict ], name : str ) -> Optional [ dict ]: \"\"\" Extract the a container from a list of containers or container definitions If not found, `None` is returned. \"\"\" for container in containers : if container . get ( \"name\" ) == name : return container return None get_prefect_container Extract the Prefect container from a list of containers or container definitions If not found, None is returned. Source code in prefect_aws/ecs.py 96 97 98 99 100 101 def get_prefect_container ( containers : List [ dict ]) -> Optional [ dict ]: \"\"\" Extract the Prefect container from a list of containers or container definitions If not found, `None` is returned. \"\"\" return get_container ( containers , PREFECT_ECS_CONTAINER_NAME )","title":"ECS"},{"location":"ecs/#prefect_aws.ecs","text":"Integrations with the Amazon Elastic Container Service. Note this module is experimental. The intefaces within may change without notice. Examples: Run a task using ECS Fargate ECSTask ( command = [ \"echo\" , \"hello world\" ]) . run () Run a task using ECS Fargate with a spot container instance ECSTask ( command = [ \"echo\" , \"hello world\" ], launch_type = \"FARGATE_SPOT\" ) . run () Run a task using ECS with an EC2 container instance ECSTask ( command = [ \"echo\" , \"hello world\" ], launch_type = \"EC2\" ) . run () Run a task on a specific VPC using ECS Fargate ECSTask ( command = [ \"echo\" , \"hello world\" ], vpc_id = \"vpc-01abcdf123456789a\" ) . run () Run a task and stream the container's output to the local terminal. Note an execution role must be provided with permissions: logs:CreateLogStream, logs:CreateLogGroup, and logs:PutLogEvents. ECSTask ( command = [ \"echo\" , \"hello world\" ], stream_output = True , execution_role_arn = \"...\" ) Run a task using an existing task definition as a base ECSTask ( command = [ \"echo\" , \"hello world\" ], task_definition_arn = \"arn:aws:ecs:...\" ) Run a task with a specific image ECSTask ( command = [ \"echo\" , \"hello world\" ], image = \"alpine:latest\" ) Run a task with custom memory and CPU requirements ECSTask ( command = [ \"echo\" , \"hello world\" ], memory = 4096 , cpu = 2048 ) Run a task with custom environment variables ECSTask ( command = [ \"echo\" , \"hello $PLANET\" ], env = { \"PLANET\" : \"earth\" }) Run a task in a specific ECS cluster ECSTask ( command = [ \"echo\" , \"hello world\" ], cluster = \"my-cluster-name\" )","title":"ecs"},{"location":"ecs/#prefect_aws.ecs.ECSTask","text":"Run a command as an ECS task. Note this block is experimental. The interface may change without notice. Source code in prefect_aws/ecs.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 994 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 class ECSTask ( Infrastructure ): \"\"\" <span class=\"badge-api experimental\"/> Run a command as an ECS task. Note this block is experimental. The interface may change without notice. \"\"\" _block_type_slug = \"ecs-task\" _block_type_name = \"ECS Task\" _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/1jbV4lceHOjGgunX15lUwT/db88e184d727f721575aeb054a37e277/aws.png?h=250\" # noqa _description = \"Run a command as an ECS task. Note this block is experimental. The interface may change without notice.\" # noqa type : Literal [ \"ecs-task\" ] = Field ( \"ecs-task\" , description = \"The slug for this task type.\" ) aws_credentials : AwsCredentials = Field ( title = \"AWS Credentials\" , default_factory = AwsCredentials , description = \"The AWS credentials to use to connect to ECS.\" , ) # Task definition settings task_definition_arn : Optional [ str ] = Field ( default = None , description = ( \"An identifier for an existing task definition to use. If fields are set \" \"on the `ECSTask` that conflict with the task definition, a new copy \" \"will be registered with the required values. \" \"Cannot be used with `task_definition`. If not provided, Prefect will \" \"generate and register a minimal task definition.\" ), ) task_definition : Optional [ dict ] = Field ( default = None , description = ( \"An ECS task definition to use. Prefect may set defaults or override \" \"fields on this task definition to match other `ECSTask` fields. \" \"Cannot be used with `task_definition_arn`. If not provided, Prefect will \" \"generate and register a minimal task definition.\" ), ) image : Optional [ str ] = Field ( default_factory = get_prefect_image_name , description = ( \"The image to use for the Prefect container in the task. If this value is \" \"not null, it will override the value in the task definition. This value \" \"defaults to a Prefect base image matching your local versions.\" ), ) auto_deregister_task_definition : bool = Field ( default = True , description = ( \"If set, any task definitions that are created by this block will be \" \"deregistered. Existing task definitions linked by ARN will never be \" \"deregistered. Deregistering a task definition does not remove it from \" \"your AWS account, instead it will be marked as INACTIVE.\" ), ) # Mixed task definition / run settings cpu : int = Field ( title = \"CPU\" , default = None , description = ( \"The amount of CPU to provide to the ECS task. Valid amounts are \" \"specified in the AWS documentation. If not provided, a default value of \" f \" { ECS_DEFAULT_CPU } will be used unless present on the task definition.\" ), ) memory : int = Field ( default = None , description = ( \"The amount of memory to provide to the ECS task. Valid amounts are \" \"specified in the AWS documentation. If not provided, a default value of \" f \" { ECS_DEFAULT_MEMORY } will be used unless present on the task definition.\" ), ) execution_role_arn : str = Field ( title = \"Execution Role ARN\" , default = None , description = ( \"An execution role to use for the task. This controls the permissions of \" \"the task when it is launching. If this value is not null, it will \" \"override the value in the task definition. An execution role must be \" \"provided to capture logs from the container.\" ), ) configure_cloudwatch_logs : bool = Field ( default = None , description = ( \"If `True`, the Prefect container will be configured to send its output \" \"to the AWS CloudWatch logs service. This functionality requires an \" \"execution role with logs:CreateLogStream, logs:CreateLogGroup, and \" \"logs:PutLogEvents permissions. The default for this field is `False` \" \"unless `stream_output` is set. \" ), ) stream_output : bool = Field ( default = None , description = ( \"If `True`, logs will be streamed from the Prefect container to the local \" \"console. Unless you have configured AWS CloudWatch logs manually on your \" \"task definition, this requires the same prerequisites outlined in \" \"`configure_cloudwatch_logs`.\" ), ) # Task run settings launch_type : Optional [ Literal [ \"FARGATE\" , \"EC2\" , \"EXTERNAL\" , \"FARGATE_SPOT\" ] ] = Field ( default = \"FARGATE\" , description = ( \"The type of ECS task run infrastructure that should be used. Note that \" \"'FARGATE_SPOT' is not a formal ECS launch type, but we will configure the \" \"proper capacity provider stategy if set here.\" ), ) vpc_id : Optional [ str ] = Field ( title = \"VPC ID\" , default = None , description = ( \"The AWS VPC to link the task run to. This is only applicable when using \" \"the 'awsvpc' network mode for your task. FARGATE tasks require this \" \"network mode, but for EC2 tasks the default network mode is 'bridge'. \" \"If using the 'awsvpc' network mode and this field is null, your default \" \"VPC will be used. If no default VPC can be found, the task run will fail.\" ), ) cluster : Optional [ str ] = Field ( default = None , description = ( \"The ECS cluster to run the task in. The ARN or name may be provided. If \" \"not provided, the default cluster will be used.\" ), ) env : Dict [ str , Optional [ str ]] = Field ( title = \"Environment Variables\" , default_factory = dict , description = ( \"Environment variables to provide to the task run. These variables are set \" \"on the Prefect container at task runtime. These will not be set on the \" \"task definition.\" ), ) task_role_arn : str = Field ( title = \"Task Role ARN\" , default = None , description = ( \"A role to attach to the task run. This controls the permissions of the \" \"task while it is running.\" ), ) # Execution settings task_start_timeout_seconds : int = Field ( default = 30 , description = ( \"The amount of time to watch for the start of the ECS task \" \"before marking it as failed. The task must enter a RUNNING state to be \" \"considered started.\" ), ) task_watch_poll_interval : float = Field ( default = 5.0 , description = ( \"The amount of time to wait between AWS API calls while monitoring the \" \"state of an ECS task.\" ), ) @root_validator ( pre = True ) def set_default_configure_cloudwatch_logs ( cls , values : dict ) -> dict : \"\"\" Streaming output generally requires CloudWatch logs to be configured. To avoid entangled arguments in the simple case, `configure_cloudwatch_logs` defaults to matching the value of `stream_output`. \"\"\" configure_cloudwatch_logs = values . get ( \"configure_cloudwatch_logs\" ) if configure_cloudwatch_logs is None : values [ \"configure_cloudwatch_logs\" ] = values . get ( \"stream_output\" ) return values @root_validator def configure_cloudwatch_logs_requires_execution_role_arn ( cls , values : dict ) -> dict : \"\"\" Enforces that an execution role arn is provided (or could be provided by a runtime task definition) when configuring logging. \"\"\" if ( values . get ( \"configure_cloudwatch_logs\" ) and not values . get ( \"execution_role_arn\" ) # Do not raise if they've linked to another task definition or provided # it without using our shortcuts and not values . get ( \"task_definition_arn\" ) and not ( values . get ( \"task_definition\" ) or {}) . get ( \"executionRoleArn\" ) ): raise ValueError ( \"An `execution_role_arn` must be provided to use \" \"`configure_cloudwatch_logs` or `stream_logs`.\" ) return values @root_validator def image_is_required ( cls , values : dict ) -> dict : \"\"\" Enforces that an image is available if the user sets it to `None`. \"\"\" if ( not values . get ( \"image\" ) and not values . get ( \"task_definition_arn\" ) # Check for an image in the task definition; whew! and not ( get_prefect_container ( ( values . get ( \"task_definition\" ) or {}) . get ( \"containerDefinitions\" , [] ) ) or {} ) . get ( \"image\" ) ): raise ValueError ( \"A value for the `image` field must be provided unless already \" \"present for the Prefect container definition a given task definition.\" ) return values @sync_compatible async def run ( self , task_status : Optional [ TaskStatus ] = None ) -> ECSTaskResult : \"\"\" Run the configured task on ECS. \"\"\" boto_session , ecs_client = await run_sync_in_worker_thread ( self . _get_session_and_client ) ( task_arn , cluster_arn , task_definition , is_new_task_definition , ) = await run_sync_in_worker_thread ( self . _create_task_and_wait_for_start , boto_session , ecs_client ) # Display a nice message indicating the command and image command = self . command or get_prefect_container ( task_definition [ \"containerDefinitions\" ] ) . get ( \"command\" , []) self . logger . info ( f \" { self . _log_prefix } : Running command { ' ' . join ( command ) !r} \" f \"in container { PREFECT_ECS_CONTAINER_NAME !r} ( { self . image } )...\" ) if task_status : task_status . started ( task_arn ) status_code = await run_sync_in_worker_thread ( self . _watch_task_and_get_exit_code , task_arn , cluster_arn , task_definition , is_new_task_definition and self . auto_deregister_task_definition , boto_session , ecs_client , ) return ECSTaskResult ( identifier = task_arn , # If the container does not start the exit code can be null but we must # still report a status code. We use a -1 to indicate a special code. status_code = status_code or - 1 , ) @property def _log_prefix ( self ) -> str : \"\"\" Internal property for generating a prefix for logs where `name` may be null \"\"\" if self . name is not None : return f \"ECSTask { self . name !r} \" else : return \"ECSTask\" def _get_session_and_client ( self ) -> Tuple [ boto3 . Session , _ECSClient ]: \"\"\" Retrieve a boto3 session and ECS client \"\"\" boto_session = self . aws_credentials . get_boto3_session () ecs_client = boto_session . client ( \"ecs\" ) return boto_session , ecs_client def _create_task_and_wait_for_start ( self , boto_session : boto3 . Session , ecs_client : _ECSClient ) -> Tuple [ str , str , dict , bool ]: \"\"\" Register the task definition, create the task run, and wait for it to start. Returns a tuple of - The task ARN - The task's cluster ARN - The task definition - A bool indicating if the task definition is newly registered \"\"\" new_task_definition_registered = False requested_task_definition = ( self . _retrieve_task_definition ( ecs_client , self . task_definition_arn ) if self . task_definition_arn else self . task_definition ) or {} task_definition_arn = requested_task_definition . get ( \"taskDefinitionArn\" , None ) task_definition = self . _prepare_task_definition ( requested_task_definition , region = ecs_client . meta . region_name ) # We must register the task definition if the arn is null or changes were made if task_definition != requested_task_definition or not task_definition_arn : self . logger . info ( f \" { self . _log_prefix } : Registering task definition...\" ) self . logger . debug ( \"Task definition payload \\n \" + yaml . dump ( task_definition )) task_definition_arn = self . _register_task_definition ( ecs_client , task_definition ) new_task_definition_registered = True if task_definition . get ( \"networkMode\" ) == \"awsvpc\" : network_config = self . _load_vpc_network_config ( self . vpc_id , boto_session ) else : network_config = None task_run = self . _prepare_task_run ( network_config = network_config , task_definition_arn = task_definition_arn , ) self . logger . info ( f \" { self . _log_prefix } : Creating task run...\" ) self . logger . debug ( \"Task run payload \\n \" + yaml . dump ( task_run )) try : task = self . _run_task ( ecs_client , task_run ) task_arn = task [ \"taskArn\" ] cluster_arn = task [ \"clusterArn\" ] except Exception as exc : self . _report_task_run_creation_failure ( task_run , exc ) # Raises an exception if the task does not start self . logger . info ( f \" { self . _log_prefix } : Waiting for task run to start...\" ) self . _wait_for_task_start ( task_arn , cluster_arn , ecs_client , timeout = self . task_start_timeout_seconds ) return task_arn , cluster_arn , task_definition , new_task_definition_registered def _watch_task_and_get_exit_code ( self , task_arn : str , cluster_arn : str , task_definition : dict , deregister_task_definition : bool , boto_session : boto3 . Session , ecs_client : _ECSClient , ) -> Optional [ int ]: \"\"\" Wait for the task run to complete and retrieve the exit code of the Prefect container. \"\"\" # Wait for completion and stream logs task = self . _wait_for_task_finish ( task_arn , cluster_arn , task_definition , ecs_client , boto_session ) if deregister_task_definition : ecs_client . deregister_task_definition ( taskDefinition = task [ \"taskDefinitionArn\" ] ) # Check the status code of the Prefect container prefect_container = get_prefect_container ( task [ \"containers\" ]) assert ( prefect_container is not None ), f \"'prefect' container missing from task: { task } \" status_code = prefect_container . get ( \"exitCode\" ) self . _report_container_status_code ( PREFECT_ECS_CONTAINER_NAME , status_code ) return status_code def preview ( self ) -> str : \"\"\" Generate a preview of the task definition and task run that will be sent to AWS. \"\"\" preview = \"\" task_definition_arn = self . task_definition_arn or \"<registered at runtime>\" if self . task_definition or not self . task_definition_arn : task_definition = self . _prepare_task_definition ( self . task_definition or {}, region = self . aws_credentials . region_name or \"<loaded from client at runtime>\" , ) preview += \"--- \\n # Task definition \\n \" preview += yaml . dump ( task_definition ) preview += \" \\n \" else : task_definition = None if task_definition and task_definition . get ( \"networkMode\" ) == \"awsvpc\" : vpc = \"the default VPC\" if not self . vpc_id else self . vpc_id network_config = { \"awsvpcConfiguration\" : f \"<loaded from { vpc } at runtime>\" } else : network_config = None task_run = self . _prepare_task_run ( network_config , task_definition_arn ) preview += \"--- \\n # Task run request \\n \" preview += yaml . dump ( task_run ) return preview def _report_container_status_code ( self , name : str , status_code : Optional [ int ] ) -> None : \"\"\" Display a log for the given container status code. \"\"\" if status_code is None : self . logger . error ( f \" { self . _log_prefix } : Task exited without reporting an exit status \" f \"for container { name !r} .\" ) elif status_code == 0 : self . logger . info ( f \" { self . _log_prefix } : Container { name !r} exited successfully.\" ) else : self . logger . warning ( f \" { self . _log_prefix } : Container { name !r} exited with non-zero exit \" f \"code { status_code } .\" ) def _report_task_run_creation_failure ( self , task_run : dict , exc : Exception ) -> None : \"\"\" Wrap common AWS task run creation failures with nicer user-facing messages. \"\"\" # AWS generates exception types at runtime so they must be captured a bit # differently than normal. if \"ClusterNotFoundException\" in str ( exc ): cluster = task_run . get ( \"cluster\" , \"default\" ) raise RuntimeError ( f \"Failed to run ECS task, cluster { cluster !r} not found. \" \"Confirm that the cluster is configured in your region.\" ) from exc elif \"No Container Instances\" in str ( exc ) and self . launch_type == \"EC2\" : cluster = task_run . get ( \"cluster\" , \"default\" ) raise RuntimeError ( f \"Failed to run ECS task, cluster { cluster !r} does not appear to \" \"have any container instances associated with it. Confirm that you \" \"have EC2 container instances available.\" ) from exc elif ( \"failed to validate logger args\" in str ( exc ) and \"AccessDeniedException\" in str ( exc ) and self . configure_cloudwatch_logs ): raise RuntimeError ( f \"Failed to run ECS task, the attached execution role does not appear \" \"to have sufficient permissions. Ensure that the execution role \" f \" { self . execution_role !r} has permissions logs:CreateLogStream, \" \"logs:CreateLogGroup, and logs:PutLogEvents.\" ) else : raise def _watch_task_run ( self , task_arn : str , cluster_arn : str , ecs_client : _ECSClient , current_status : str = \"UNKNOWN\" , until_status : str = None , timeout : int = None , ) -> Generator [ None , None , dict ]: \"\"\" Watches an ECS task run by querying every `poll_interval` seconds. After each query, the retrieved task is yielded. This function returns when the task run reaches a STOPPED status or the provided `until_status`. Emits a log each time the status changes. \"\"\" last_status = status = current_status t0 = time . time () while status != until_status : task = ecs_client . describe_tasks ( tasks = [ task_arn ], cluster = cluster_arn )[ \"tasks\" ][ 0 ] status = task [ \"lastStatus\" ] if status != last_status : self . logger . info ( f \" { self . _log_prefix } : Status is { status } .\" ) yield task # No point in continuing if the status is final if status == \"STOPPED\" : break last_status = status elapsed_time = time . time () - t0 if timeout is not None and elapsed_time > timeout : raise RuntimeError ( f \"Timed out after { elapsed_time } s while watching task for status \" \"{until_status or 'STOPPED'}\" ) time . sleep ( self . task_watch_poll_interval ) def _wait_for_task_start ( self , task_arn : str , cluster_arn : str , ecs_client : _ECSClient , timeout : int ) -> dict : \"\"\" Waits for an ECS task run to reach a RUNNING status. If a STOPPED status is reached instead, an exception is raised indicating the reason that the task run did not start. \"\"\" for task in self . _watch_task_run ( task_arn , cluster_arn , ecs_client , until_status = \"RUNNING\" , timeout = timeout ): # TODO: It is possible that the task has passed _through_ a RUNNING # status during the polling interval. In this case, there is not an # exception to raise. if task [ \"lastStatus\" ] == \"STOPPED\" : code = task . get ( \"stopCode\" ) reason = task . get ( \"stoppedReason\" ) # Generate a dynamic exception type from the AWS name raise type ( code , ( RuntimeError ,), {})( reason ) return task def _wait_for_task_finish ( self , task_arn : str , cluster_arn : str , task_definition : dict , ecs_client : _ECSClient , boto_session : boto3 . Session , ): \"\"\" Watch an ECS task until it reaches a STOPPED status. If configured, logs from the Prefect container are streamed to stderr. Returns a description of the task on completion. \"\"\" can_stream_output = False if self . stream_output : container_def = get_prefect_container ( task_definition [ \"containerDefinitions\" ] ) if not container_def : self . logger . warning ( f \" { self . _log_prefix } : Prefect container definition not found in \" \"task definition. Output cannot be streamed.\" ) elif not container_def . get ( \"logConfiguration\" ): self . logger . warning ( f \" { self . _log_prefix } : Logging configuration not found on task. \" \"Output cannot be streamed.\" ) elif not container_def [ \"logConfiguration\" ] . get ( \"logDriver\" ) == \"awslogs\" : self . logger . warning ( f \" { self . _log_prefix } : Logging configuration uses unsupported \" \" driver {container_def['logConfiguration'].get('logDriver')!r}. \" \"Output cannot be streamed.\" ) else : # Prepare to stream the output log_config = container_def [ \"logConfiguration\" ][ \"options\" ] logs_client = boto_session . client ( \"logs\" ) can_stream_output = True # Track the last log timestamp to prevent double display last_log_timestamp : Optional [ int ] = None # Determine the name of the stream as \"prefix/family/run\" stream_name = \"/\" . join ( [ log_config [ \"awslogs-stream-prefix\" ], task_definition [ \"family\" ], task_arn . rsplit ( \"/\" )[ - 1 ], ] ) self . logger . info ( f \" { self . _log_prefix } : Streaming output from container \" f \" { PREFECT_ECS_CONTAINER_NAME !r} ...\" ) for task in self . _watch_task_run ( task_arn , cluster_arn , ecs_client , current_status = \"RUNNING\" ): if self . stream_output and can_stream_output : # On each poll for task run status, also retrieve available logs last_log_timestamp = self . _stream_available_logs ( logs_client , log_group = log_config [ \"awslogs-group\" ], log_stream = stream_name , last_log_timestamp = last_log_timestamp , ) return task def _stream_available_logs ( self , logs_client : Any , log_group : str , log_stream : str , last_log_timestamp : Optional [ int ] = None , ) -> Optional [ int ]: \"\"\" Stream logs from the given log group and stream since the last log timestamp. Will continue on paginated responses until all logs are returned. Returns the last log timestamp which can be used to call this method in the future. \"\"\" last_log_stream_token = \"NO-TOKEN\" next_log_stream_token = None # AWS will return the same token that we send once the end of the paginated # response is reached while last_log_stream_token != next_log_stream_token : last_log_stream_token = next_log_stream_token request = { \"logGroupName\" : log_group , \"logStreamName\" : log_stream , } if last_log_stream_token is not None : request [ \"nextToken\" ] = last_log_stream_token if last_log_timestamp is not None : # Bump the timestamp by one ms to avoid retrieving the last log again request [ \"startTime\" ] = last_log_timestamp + 1 response = logs_client . get_log_events ( ** request ) log_events = response [ \"events\" ] for log_event in log_events : # TODO: This doesn't forward to the local logger, which can be # bad for customizing handling and understanding where the # log is coming from, but it avoid nesting logger information # when the content is output from a Prefect logger on the # running infrastructure print ( log_event [ \"message\" ], file = sys . stderr ) if ( last_log_timestamp is None or log_event [ \"timestamp\" ] > last_log_timestamp ): last_log_timestamp = log_event [ \"timestamp\" ] next_log_stream_token = response . get ( \"nextForwardToken\" ) return last_log_timestamp def _retrieve_task_definition ( self , ecs_client : _ECSClient , task_definition_arn : str ): \"\"\" Retrieve an existing task definition from AWS. \"\"\" self . logger . info ( f \" { self . _log_prefix } : \" f \"Retrieving task definition { task_definition_arn !r} ...\" ) response = ecs_client . describe_task_definition ( taskDefinition = task_definition_arn ) return response [ \"taskDefinition\" ] def _register_task_definition ( self , ecs_client : _ECSClient , task_definition : dict ) -> str : \"\"\" Register a new task definition with AWS. \"\"\" # TODO: Consider including a global cache for this task definition since # registration of task definitions is frequently rate limited task_definition_request = copy . deepcopy ( task_definition ) # We need to remove some fields here if copying an existing task definition if self . task_definition_arn : task_definition_request . pop ( \"compatibilities\" , None ) task_definition_request . pop ( \"taskDefinitionArn\" , None ) task_definition_request . pop ( \"revision\" , None ) task_definition_request . pop ( \"status\" , None ) response = ecs_client . register_task_definition ( ** task_definition_request ) return response [ \"taskDefinition\" ][ \"taskDefinitionArn\" ] def _prepare_task_definition ( self , task_definition : dict , region : str ) -> dict : \"\"\" Prepare a task definition by inferring any defaults and merging overrides. \"\"\" task_definition = copy . deepcopy ( task_definition ) # Configure the Prefect runtime container task_definition . setdefault ( \"containerDefinitions\" , []) container = get_prefect_container ( task_definition [ \"containerDefinitions\" ]) if container is None : container = { \"name\" : PREFECT_ECS_CONTAINER_NAME } task_definition [ \"containerDefinitions\" ] . append ( container ) if self . image : container [ \"image\" ] = self . image # Remove any keys that have been explicitly \"unset\" unset_keys = { key for key , value in self . env . items () if value is None } for item in tuple ( container . get ( \"environment\" , [])): if item [ \"name\" ] in unset_keys : container [ \"environment\" ] . remove ( item ) if self . configure_cloudwatch_logs : container [ \"logConfiguration\" ] = { \"logDriver\" : \"awslogs\" , \"options\" : { \"awslogs-create-group\" : \"true\" , \"awslogs-group\" : \"prefect\" , \"awslogs-region\" : region , \"awslogs-stream-prefix\" : self . name or \"prefect\" , }, } task_definition . setdefault ( \"family\" , \"prefect\" ) # CPU and memory are required in some cases, retrieve the value to use cpu = self . cpu or task_definition . get ( \"cpu\" ) or ECS_DEFAULT_CPU memory = self . memory or task_definition . get ( \"memory\" ) or ECS_DEFAULT_MEMORY if self . launch_type == \"FARGATE\" or self . launch_type == \"FARGATE_SPOT\" : # Task level memory and cpu are required when using fargate task_definition [ \"cpu\" ] = str ( cpu ) task_definition [ \"memory\" ] = str ( memory ) # The FARGATE compatibility is required if it will be used as as launch type requires_compatibilities = task_definition . setdefault ( \"requiresCompatibilities\" , [] ) if \"FARGATE\" not in requires_compatibilities : task_definition [ \"requiresCompatibilities\" ] . append ( \"FARGATE\" ) # Only the 'awsvpc' network mode is supported when using FARGATE # However, we will not enforce that here if the user has set it network_mode = task_definition . setdefault ( \"networkMode\" , \"awsvpc\" ) if network_mode != \"awsvpc\" : warnings . warn ( f \"Found network mode { network_mode !r} which is not compatible with \" f \"launch type { self . launch_type !r} . Use either the 'EC2' launch \" \"type or the 'awsvpc' network mode.\" ) elif self . launch_type == \"EC2\" : # Container level memory and cpu are required when using ec2 container . setdefault ( \"cpu\" , int ( cpu )) container . setdefault ( \"memory\" , int ( memory )) if self . execution_role_arn and not self . task_definition_arn : task_definition [ \"executionRoleArn\" ] = self . execution_role_arn if self . configure_cloudwatch_logs and not task_definition . get ( \"executionRoleArn\" ): raise ValueError ( \"An execution role arn must be set on the task definition to use \" \"`configure_cloudwatch_logs` or `stream_logs` but no execution role \" \"was found on the task definition.\" ) return task_definition def _prepare_task_run_overrides ( self ) -> dict : \"\"\" Prepare the 'overrides' payload for a task run request. \"\"\" overrides = { \"containerOverrides\" : [ { \"name\" : PREFECT_ECS_CONTAINER_NAME , \"environment\" : [ { \"name\" : key , \"value\" : value } for key , value in { ** self . _base_environment (), ** self . env , } . items () if value is not None ], } ], } prefect_container_overrides = overrides [ \"containerOverrides\" ][ 0 ] if self . command : prefect_container_overrides [ \"command\" ] = self . command if self . execution_role_arn : overrides [ \"executionRoleArn\" ] = self . execution_role_arn if self . task_role_arn : overrides [ \"taskRoleArn\" ] = self . task_role_arn if self . memory : overrides [ \"memory\" ] = str ( self . memory ) prefect_container_overrides . setdefault ( \"memory\" , self . memory ) if self . cpu : overrides [ \"cpu\" ] = str ( self . cpu ) prefect_container_overrides . setdefault ( \"cpu\" , self . cpu ) return overrides def _load_vpc_network_config ( self , vpc_id : Optional [ str ], boto_session : boto3 . Session ) -> dict : \"\"\" Load settings from a specific VPC or the default VPC and generate a task run request's network configuration. \"\"\" ec2_client = boto_session . client ( \"ec2\" ) vpc_message = \"the default VPC\" if not vpc_id else f \"VPC with ID { vpc_id } \" if not vpc_id : # Retrieve the default VPC describe = { \"Filters\" : [{ \"Name\" : \"isDefault\" , \"Values\" : [ \"true\" ]}]} else : describe = { \"VpcIds\" : [ vpc_id ]} vpcs = ec2_client . describe_vpcs ( ** describe )[ \"Vpcs\" ] if not vpcs : help_message = ( \"Pass an explicit `vpc_id` or configure a default VPC.\" if not vpc_id else \"Check that the VPC exists in the current region.\" ) raise ValueError ( f \"Failed to find { vpc_message } . \" \"Network configuration cannot be inferred. \" + help_message ) vpc_id = vpcs [ 0 ][ \"VpcId\" ] subnets = ec2_client . describe_subnets ( Filters = [{ \"Name\" : \"vpc-id\" , \"Values\" : [ vpc_id ]}] )[ \"Subnets\" ] if not subnets : raise ValueError ( f \"Failed to find subnets for { vpc_message } . \" \"Network configuration cannot be inferred.\" ) return { \"awsvpcConfiguration\" : { \"subnets\" : [ s [ \"SubnetId\" ] for s in subnets ], \"assignPublicIp\" : \"ENABLED\" , } } def _prepare_task_run ( self , network_config : Optional [ dict ], task_definition_arn : str , ) -> dict : \"\"\" Prepare a task run request payload. \"\"\" task_run = { \"overrides\" : self . _prepare_task_run_overrides (), \"tags\" : [ { \"key\" : key , \"value\" : value } for key , value in self . labels . items () ], \"taskDefinition\" : task_definition_arn , } if self . cluster : task_run [ \"cluster\" ] = self . cluster if self . launch_type : if self . launch_type == \"FARGATE_SPOT\" : task_run [ \"capacityProviderStrategy\" ] = [ { \"capacityProvider\" : \"FARGATE_SPOT\" , \"weight\" : 1 } ] else : task_run [ \"launchType\" ] = self . launch_type if network_config : task_run [ \"networkConfiguration\" ] = network_config return task_run def _run_task ( self , ecs_client : _ECSClient , task_run : dict ): \"\"\" Run the task using the ECS client. This is isolated as a separate method for testing purposes. \"\"\" return ecs_client . run_task ( ** task_run )[ \"tasks\" ][ 0 ]","title":"ECSTask"},{"location":"ecs/#prefect_aws.ecs.ECSTask.configure_cloudwatch_logs_requires_execution_role_arn","text":"Enforces that an execution role arn is provided (or could be provided by a runtime task definition) when configuring logging. Source code in prefect_aws/ecs.py 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 @root_validator def configure_cloudwatch_logs_requires_execution_role_arn ( cls , values : dict ) -> dict : \"\"\" Enforces that an execution role arn is provided (or could be provided by a runtime task definition) when configuring logging. \"\"\" if ( values . get ( \"configure_cloudwatch_logs\" ) and not values . get ( \"execution_role_arn\" ) # Do not raise if they've linked to another task definition or provided # it without using our shortcuts and not values . get ( \"task_definition_arn\" ) and not ( values . get ( \"task_definition\" ) or {}) . get ( \"executionRoleArn\" ) ): raise ValueError ( \"An `execution_role_arn` must be provided to use \" \"`configure_cloudwatch_logs` or `stream_logs`.\" ) return values","title":"configure_cloudwatch_logs_requires_execution_role_arn()"},{"location":"ecs/#prefect_aws.ecs.ECSTask.image_is_required","text":"Enforces that an image is available if the user sets it to None . Source code in prefect_aws/ecs.py 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 @root_validator def image_is_required ( cls , values : dict ) -> dict : \"\"\" Enforces that an image is available if the user sets it to `None`. \"\"\" if ( not values . get ( \"image\" ) and not values . get ( \"task_definition_arn\" ) # Check for an image in the task definition; whew! and not ( get_prefect_container ( ( values . get ( \"task_definition\" ) or {}) . get ( \"containerDefinitions\" , [] ) ) or {} ) . get ( \"image\" ) ): raise ValueError ( \"A value for the `image` field must be provided unless already \" \"present for the Prefect container definition a given task definition.\" ) return values","title":"image_is_required()"},{"location":"ecs/#prefect_aws.ecs.ECSTask.preview","text":"Generate a preview of the task definition and task run that will be sent to AWS. Source code in prefect_aws/ecs.py 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 def preview ( self ) -> str : \"\"\" Generate a preview of the task definition and task run that will be sent to AWS. \"\"\" preview = \"\" task_definition_arn = self . task_definition_arn or \"<registered at runtime>\" if self . task_definition or not self . task_definition_arn : task_definition = self . _prepare_task_definition ( self . task_definition or {}, region = self . aws_credentials . region_name or \"<loaded from client at runtime>\" , ) preview += \"--- \\n # Task definition \\n \" preview += yaml . dump ( task_definition ) preview += \" \\n \" else : task_definition = None if task_definition and task_definition . get ( \"networkMode\" ) == \"awsvpc\" : vpc = \"the default VPC\" if not self . vpc_id else self . vpc_id network_config = { \"awsvpcConfiguration\" : f \"<loaded from { vpc } at runtime>\" } else : network_config = None task_run = self . _prepare_task_run ( network_config , task_definition_arn ) preview += \"--- \\n # Task run request \\n \" preview += yaml . dump ( task_run ) return preview","title":"preview()"},{"location":"ecs/#prefect_aws.ecs.ECSTask.run","text":"Run the configured task on ECS. Source code in prefect_aws/ecs.py 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 @sync_compatible async def run ( self , task_status : Optional [ TaskStatus ] = None ) -> ECSTaskResult : \"\"\" Run the configured task on ECS. \"\"\" boto_session , ecs_client = await run_sync_in_worker_thread ( self . _get_session_and_client ) ( task_arn , cluster_arn , task_definition , is_new_task_definition , ) = await run_sync_in_worker_thread ( self . _create_task_and_wait_for_start , boto_session , ecs_client ) # Display a nice message indicating the command and image command = self . command or get_prefect_container ( task_definition [ \"containerDefinitions\" ] ) . get ( \"command\" , []) self . logger . info ( f \" { self . _log_prefix } : Running command { ' ' . join ( command ) !r} \" f \"in container { PREFECT_ECS_CONTAINER_NAME !r} ( { self . image } )...\" ) if task_status : task_status . started ( task_arn ) status_code = await run_sync_in_worker_thread ( self . _watch_task_and_get_exit_code , task_arn , cluster_arn , task_definition , is_new_task_definition and self . auto_deregister_task_definition , boto_session , ecs_client , ) return ECSTaskResult ( identifier = task_arn , # If the container does not start the exit code can be null but we must # still report a status code. We use a -1 to indicate a special code. status_code = status_code or - 1 , )","title":"run()"},{"location":"ecs/#prefect_aws.ecs.ECSTask.set_default_configure_cloudwatch_logs","text":"Streaming output generally requires CloudWatch logs to be configured. To avoid entangled arguments in the simple case, configure_cloudwatch_logs defaults to matching the value of stream_output . Source code in prefect_aws/ecs.py 289 290 291 292 293 294 295 296 297 298 299 300 @root_validator ( pre = True ) def set_default_configure_cloudwatch_logs ( cls , values : dict ) -> dict : \"\"\" Streaming output generally requires CloudWatch logs to be configured. To avoid entangled arguments in the simple case, `configure_cloudwatch_logs` defaults to matching the value of `stream_output`. \"\"\" configure_cloudwatch_logs = values . get ( \"configure_cloudwatch_logs\" ) if configure_cloudwatch_logs is None : values [ \"configure_cloudwatch_logs\" ] = values . get ( \"stream_output\" ) return values","title":"set_default_configure_cloudwatch_logs()"},{"location":"ecs/#prefect_aws.ecs.ECSTaskResult","text":"The result of a run of an ECS task Source code in prefect_aws/ecs.py 87 88 class ECSTaskResult ( InfrastructureResult ): \"\"\"The result of a run of an ECS task\"\"\"","title":"ECSTaskResult"},{"location":"ecs/#prefect_aws.ecs.get_container","text":"Extract the a container from a list of containers or container definitions If not found, None is returned. Source code in prefect_aws/ecs.py 104 105 106 107 108 109 110 111 112 def get_container ( containers : List [ dict ], name : str ) -> Optional [ dict ]: \"\"\" Extract the a container from a list of containers or container definitions If not found, `None` is returned. \"\"\" for container in containers : if container . get ( \"name\" ) == name : return container return None","title":"get_container()"},{"location":"ecs/#prefect_aws.ecs.get_prefect_container","text":"Extract the Prefect container from a list of containers or container definitions If not found, None is returned. Source code in prefect_aws/ecs.py 96 97 98 99 100 101 def get_prefect_container ( containers : List [ dict ]) -> Optional [ dict ]: \"\"\" Extract the Prefect container from a list of containers or container definitions If not found, `None` is returned. \"\"\" return get_container ( containers , PREFECT_ECS_CONTAINER_NAME )","title":"get_prefect_container()"},{"location":"s3/","text":"prefect_aws.s3 Tasks for interacting with AWS S3 S3Bucket Block used to store data using AWS S3 or S3-compatible object storage like MinIO. Parameters: Name Type Description Default bucket_name Name of your bucket. required aws_credentials A block containing your credentials (choose this or minio_credentials). required minio_credentials A block containing your credentials (choose this or aws_credentials). required basepath Used when you don't want to read/write at base level. required endpoint_url Used for non-AWS configuration. When unspecified, defaults to AWS. required Example Load stored S3Bucket configuration: from prefect_aws.s3 import S3Bucket s3bucket_block = S3Bucket . load ( \"BLOCK_NAME\" ) Source code in prefect_aws/s3.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 class S3Bucket ( ReadableFileSystem , WritableFileSystem ): \"\"\" Block used to store data using AWS S3 or S3-compatible object storage like MinIO. Args: bucket_name: Name of your bucket. aws_credentials: A block containing your credentials (choose this or minio_credentials). minio_credentials: A block containing your credentials (choose this or aws_credentials). basepath: Used when you don't want to read/write at base level. endpoint_url: Used for non-AWS configuration. When unspecified, defaults to AWS. Example: Load stored S3Bucket configuration: ```python from prefect_aws.s3 import S3Bucket s3bucket_block = S3Bucket.load(\"BLOCK_NAME\") ``` \"\"\" # change _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/uPezmBzEv4moXKdQJ3YyL/a1f029b423cf67f474d1eee33c1463d7/pngwing.com.png?h=250\" # noqa _block_type_name = \"S3 Bucket\" bucket_name : str minio_credentials : Optional [ MinIOCredentials ] aws_credentials : Optional [ AwsCredentials ] basepath : Optional [ Path ] endpoint_url : Optional [ str ] @validator ( \"basepath\" , pre = True ) def cast_pathlib ( cls , value ): \"\"\" If basepath provided, it means we aren't writing to the root directory of the bucket. We need to ensure that it is a valid path. This is called when the S3Bucket block is instantiated. \"\"\" if isinstance ( value , Path ): return str ( value ) return value @root_validator ( pre = True ) def check_credentials ( cls , values ): \"\"\" Ensure exactly 1 of 2 optional credentials fields has been provided by user. \"\"\" minio_creds_exist = bool ( values . get ( \"minio_credentials\" )) aws_creds_exist = bool ( values . get ( \"aws_credentials\" )) # if both credentials fields provided if minio_creds_exist and aws_creds_exist : raise ValueError ( \"S3Bucket accepts a minio_credentials field or an\" \"aws_credentials field but not both.\" ) # if neither credentials fields provided if not minio_creds_exist and not aws_creds_exist : raise ValueError ( \"S3Bucket requires either a minio_credentials\" \"field or an aws_credentials field.\" ) return values def _resolve_path ( self , path : str ) -> Path : \"\"\" A helper function used in write_path to join `self.basepath` and `path`. Args: path: Name of the key, e.g. \"file1\". Each object in your bucket has a unique key (or key name). \"\"\" path = path or str ( uuid4 ()) # If basepath provided, it means we won't write to the root dir of # the bucket. So we need to add it on the front of the path. path = str ( Path ( self . basepath ) / path ) if self . basepath else path return path def _get_s3_client ( self ) -> boto3 . client : \"\"\" Authenticate MinIO credentials or AWS credentials and return an S3 client. This is a helper function called by read_path() or write_path(). \"\"\" if self . minio_credentials : s3_client = self . minio_credentials . get_boto3_session () . client ( service_name = \"s3\" , endpoint_url = self . endpoint_url ) elif self . aws_credentials : s3_client = self . aws_credentials . get_boto3_session () . client ( service_name = \"s3\" ) return s3_client async def read_path ( self , path : str ) -> bytes : \"\"\" Read specified path from S3 and return contents. Provide the entire path to the key in S3. Args: path: Entire path to (and including) the key. Example: Read \"subfolder/file1\" contents from an S3 bucket named \"bucket\": ```python from prefect_aws import AwsCredentials from prefect_aws.s3 import S3Bucket aws_creds = AwsCredentials( aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY ) s3_bucket_block = S3Bucket( bucket_name=\"bucket\", aws_credentials=aws_creds, basepath=\"subfolder\" ) key_contents = s3_bucket_block.read_path(path=\"subfolder/file1\") ``` \"\"\" return await run_sync_in_worker_thread ( self . _read_sync , path ) def _read_sync ( self , key : str ) -> bytes : \"\"\" Called by read_path(). Creates an S3 client and retrieves the contents from a specified path. \"\"\" s3_client = self . _get_s3_client () with io . BytesIO () as stream : s3_client . download_fileobj ( Bucket = self . bucket_name , Key = key , Fileobj = stream ) stream . seek ( 0 ) output = stream . read () return output async def write_path ( self , path : str , content : bytes ) -> str : \"\"\" Writes to an S3 bucket. Args: path: The key name. Each object in your bucket has a unique key (or key name). content: What you are uploading to S3. Example: Write data to the path `dogs/small_dogs/havanese` in an S3 Bucket: ```python from prefect_aws import MinioCredentials from prefect_aws.s3 import S3Bucket minio_creds = MinIOCredentials( minio_root_user = \"minioadmin\", minio_root_password = \"minioadmin\", ) s3_bucket_block = S3Bucket( bucket_name=\"bucket\", minio_credentials=minio_creds, basepath=\"dogs/smalldogs\", endpoint_url=\"http://localhost:9000\", ) s3_havanese_path = s3_bucket_block.write_path(path=\"havanese\", content=data) ``` \"\"\" path = self . _resolve_path ( path ) await run_sync_in_worker_thread ( self . _write_sync , path , content ) return path def _write_sync ( self , key : str , data : bytes ) -> None : \"\"\" Called by write_path(). Creates an S3 client and uploads a file object. \"\"\" s3_client = self . _get_s3_client () with io . BytesIO ( data ) as stream : s3_client . upload_fileobj ( Fileobj = stream , Bucket = self . bucket_name , Key = key ) cast_pathlib If basepath provided, it means we aren't writing to the root directory of the bucket. We need to ensure that it is a valid path. This is called when the S3Bucket block is instantiated. Source code in prefect_aws/s3.py 266 267 268 269 270 271 272 273 274 275 276 277 @validator ( \"basepath\" , pre = True ) def cast_pathlib ( cls , value ): \"\"\" If basepath provided, it means we aren't writing to the root directory of the bucket. We need to ensure that it is a valid path. This is called when the S3Bucket block is instantiated. \"\"\" if isinstance ( value , Path ): return str ( value ) return value check_credentials Ensure exactly 1 of 2 optional credentials fields has been provided by user. Source code in prefect_aws/s3.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 @root_validator ( pre = True ) def check_credentials ( cls , values ): \"\"\" Ensure exactly 1 of 2 optional credentials fields has been provided by user. \"\"\" minio_creds_exist = bool ( values . get ( \"minio_credentials\" )) aws_creds_exist = bool ( values . get ( \"aws_credentials\" )) # if both credentials fields provided if minio_creds_exist and aws_creds_exist : raise ValueError ( \"S3Bucket accepts a minio_credentials field or an\" \"aws_credentials field but not both.\" ) # if neither credentials fields provided if not minio_creds_exist and not aws_creds_exist : raise ValueError ( \"S3Bucket requires either a minio_credentials\" \"field or an aws_credentials field.\" ) return values read_path async Read specified path from S3 and return contents. Provide the entire path to the key in S3. Parameters: Name Type Description Default path str Entire path to (and including) the key. required Example Read \"subfolder/file1\" contents from an S3 bucket named \"bucket\": from prefect_aws import AwsCredentials from prefect_aws.s3 import S3Bucket aws_creds = AwsCredentials ( aws_access_key_id = AWS_ACCESS_KEY_ID , aws_secret_access_key = AWS_SECRET_ACCESS_KEY ) s3_bucket_block = S3Bucket ( bucket_name = \"bucket\" , aws_credentials = aws_creds , basepath = \"subfolder\" ) key_contents = s3_bucket_block . read_path ( path = \"subfolder/file1\" ) Source code in prefect_aws/s3.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 async def read_path ( self , path : str ) -> bytes : \"\"\" Read specified path from S3 and return contents. Provide the entire path to the key in S3. Args: path: Entire path to (and including) the key. Example: Read \"subfolder/file1\" contents from an S3 bucket named \"bucket\": ```python from prefect_aws import AwsCredentials from prefect_aws.s3 import S3Bucket aws_creds = AwsCredentials( aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY ) s3_bucket_block = S3Bucket( bucket_name=\"bucket\", aws_credentials=aws_creds, basepath=\"subfolder\" ) key_contents = s3_bucket_block.read_path(path=\"subfolder/file1\") ``` \"\"\" return await run_sync_in_worker_thread ( self . _read_sync , path ) write_path async Writes to an S3 bucket. Parameters: Name Type Description Default path str The key name. Each object in your bucket has a unique key (or key name). required content bytes What you are uploading to S3. required Example Write data to the path dogs/small_dogs/havanese in an S3 Bucket: from prefect_aws import MinioCredentials from prefect_aws.s3 import S3Bucket minio_creds = MinIOCredentials ( minio_root_user = \"minioadmin\" , minio_root_password = \"minioadmin\" , ) s3_bucket_block = S3Bucket ( bucket_name = \"bucket\" , minio_credentials = minio_creds , basepath = \"dogs/smalldogs\" , endpoint_url = \"http://localhost:9000\" , ) s3_havanese_path = s3_bucket_block . write_path ( path = \"havanese\" , content = data ) Source code in prefect_aws/s3.py 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 async def write_path ( self , path : str , content : bytes ) -> str : \"\"\" Writes to an S3 bucket. Args: path: The key name. Each object in your bucket has a unique key (or key name). content: What you are uploading to S3. Example: Write data to the path `dogs/small_dogs/havanese` in an S3 Bucket: ```python from prefect_aws import MinioCredentials from prefect_aws.s3 import S3Bucket minio_creds = MinIOCredentials( minio_root_user = \"minioadmin\", minio_root_password = \"minioadmin\", ) s3_bucket_block = S3Bucket( bucket_name=\"bucket\", minio_credentials=minio_creds, basepath=\"dogs/smalldogs\", endpoint_url=\"http://localhost:9000\", ) s3_havanese_path = s3_bucket_block.write_path(path=\"havanese\", content=data) ``` \"\"\" path = self . _resolve_path ( path ) await run_sync_in_worker_thread ( self . _write_sync , path , content ) return path s3_download async Downloads an object with a given key from a given S3 bucket. Parameters: Name Type Description Default bucket str Name of bucket to download object from. Required if a default value was not supplied when creating the task. required key str Key of object to download. Required if a default value was not supplied when creating the task. required aws_credentials AwsCredentials Credentials to use for authentication with AWS. required aws_client_parameters AwsClientParameters Custom parameter for the boto3 client initialization.. AwsClientParameters() Returns: Type Description bytes A bytes representation of the downloaded object. Example Download a file from an S3 bucket: from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.s3 import s3_download @flow async def example_s3_download_flow (): aws_credentials = AwsCredentials ( aws_access_key_id = \"acccess_key_id\" , aws_secret_access_key = \"secret_access_key\" ) data = await s3_download ( bucket = \"bucket\" , key = \"key\" , aws_credentials = aws_credentials , ) example_s3_download_flow () Source code in prefect_aws/s3.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @task async def s3_download ( bucket : str , key : str , aws_credentials : AwsCredentials , aws_client_parameters : AwsClientParameters = AwsClientParameters (), ) -> bytes : \"\"\" Downloads an object with a given key from a given S3 bucket. Args: bucket: Name of bucket to download object from. Required if a default value was not supplied when creating the task. key: Key of object to download. Required if a default value was not supplied when creating the task. aws_credentials: Credentials to use for authentication with AWS. aws_client_parameters: Custom parameter for the boto3 client initialization.. Returns: A `bytes` representation of the downloaded object. Example: Download a file from an S3 bucket: ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.s3 import s3_download @flow async def example_s3_download_flow(): aws_credentials = AwsCredentials( aws_access_key_id=\"acccess_key_id\", aws_secret_access_key=\"secret_access_key\" ) data = await s3_download( bucket=\"bucket\", key=\"key\", aws_credentials=aws_credentials, ) example_s3_download_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Downloading object from bucket %s with key %s \" , bucket , key ) s3_client = aws_credentials . get_boto3_session () . client ( \"s3\" , ** aws_client_parameters . get_params_override () ) stream = io . BytesIO () await run_sync_in_worker_thread ( s3_client . download_fileobj , Bucket = bucket , Key = key , Fileobj = stream ) stream . seek ( 0 ) output = stream . read () return output s3_list_objects async Lists details of objects in a given S3 bucket. Parameters: Name Type Description Default bucket str Name of bucket to list items from. Required if a default value was not supplied when creating the task. required aws_credentials AwsCredentials Credentials to use for authentication with AWS. required aws_client_parameters AwsClientParameters Custom parameter for the boto3 client initialization.. AwsClientParameters() prefix str Used to filter objects with keys starting with the specified prefix. '' delimiter str Character used to group keys of listed objects. '' page_size Optional [ int ] Number of objects to return in each request to the AWS API. None max_items Optional [ int ] Maximum number of objects that to be returned by task. None jmespath_query Optional [ str ] Query used to filter objects based on object attributes refer to the boto3 docs for more information on how to construct queries. None Returns: Type Description List [ Dict [ str , Any ]] A list of dictionaries containing information about the objects retrieved. Refer to the boto3 docs for an example response. Example List all objects in a bucket: from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.s3 import s3_list_objects @flow async def example_s3_list_objects_flow (): aws_credentials = AwsCredentials ( aws_access_key_id = \"acccess_key_id\" , aws_secret_access_key = \"secret_access_key\" ) objects = await s3_list_objects ( bucket = \"data_bucket\" , aws_credentials = aws_credentials ) example_s3_list_objects_flow () Source code in prefect_aws/s3.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 @task async def s3_list_objects ( bucket : str , aws_credentials : AwsCredentials , aws_client_parameters : AwsClientParameters = AwsClientParameters (), prefix : str = \"\" , delimiter : str = \"\" , page_size : Optional [ int ] = None , max_items : Optional [ int ] = None , jmespath_query : Optional [ str ] = None , ) -> List [ Dict [ str , Any ]]: \"\"\" Lists details of objects in a given S3 bucket. Args: bucket: Name of bucket to list items from. Required if a default value was not supplied when creating the task. aws_credentials: Credentials to use for authentication with AWS. aws_client_parameters: Custom parameter for the boto3 client initialization.. prefix: Used to filter objects with keys starting with the specified prefix. delimiter: Character used to group keys of listed objects. page_size: Number of objects to return in each request to the AWS API. max_items: Maximum number of objects that to be returned by task. jmespath_query: Query used to filter objects based on object attributes refer to the [boto3 docs](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/paginators.html#filtering-results-with-jmespath) for more information on how to construct queries. Returns: A list of dictionaries containing information about the objects retrieved. Refer to the boto3 docs for an example response. Example: List all objects in a bucket: ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.s3 import s3_list_objects @flow async def example_s3_list_objects_flow(): aws_credentials = AwsCredentials( aws_access_key_id=\"acccess_key_id\", aws_secret_access_key=\"secret_access_key\" ) objects = await s3_list_objects( bucket=\"data_bucket\", aws_credentials=aws_credentials ) example_s3_list_objects_flow() ``` \"\"\" # noqa E501 logger = get_run_logger () logger . info ( \"Listing objects in bucket %s with prefix %s \" , bucket , prefix ) s3_client = aws_credentials . get_boto3_session () . client ( \"s3\" , ** aws_client_parameters . get_params_override () ) paginator = s3_client . get_paginator ( \"list_objects_v2\" ) page_iterator = paginator . paginate ( Bucket = bucket , Prefix = prefix , Delimiter = delimiter , PaginationConfig = { \"PageSize\" : page_size , \"MaxItems\" : max_items }, ) if jmespath_query : page_iterator = page_iterator . search ( f \" { jmespath_query } | {{ Contents: @ }} \" ) return await run_sync_in_worker_thread ( _list_objects_sync , page_iterator ) s3_upload async Uploads data to an S3 bucket. Parameters: Name Type Description Default data bytes Bytes representation of data to upload to S3. required bucket str Name of bucket to upload data to. Required if a default value was not supplied when creating the task. required aws_credentials AwsCredentials Credentials to use for authentication with AWS. required aws_client_parameters AwsClientParameters Custom parameter for the boto3 client initialization.. AwsClientParameters() key Optional [ str ] Key of object to download. Defaults to a UUID string. None Returns: Type Description str The key of the uploaded object Example Read and upload a file to an S3 bucket: from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.s3 import s3_upload @flow async def example_s3_upload_flow (): aws_credentials = AwsCredentials ( aws_access_key_id = \"acccess_key_id\" , aws_secret_access_key = \"secret_access_key\" ) with open ( \"data.csv\" , \"rb\" ) as file : key = await s3_upload ( bucket = \"bucket\" , key = \"data.csv\" , data = file . read (), aws_credentials = aws_credentials , ) example_s3_upload_flow () Source code in prefect_aws/s3.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 @task async def s3_upload ( data : bytes , bucket : str , aws_credentials : AwsCredentials , aws_client_parameters : AwsClientParameters = AwsClientParameters (), key : Optional [ str ] = None , ) -> str : \"\"\" Uploads data to an S3 bucket. Args: data: Bytes representation of data to upload to S3. bucket: Name of bucket to upload data to. Required if a default value was not supplied when creating the task. aws_credentials: Credentials to use for authentication with AWS. aws_client_parameters: Custom parameter for the boto3 client initialization.. key: Key of object to download. Defaults to a UUID string. Returns: The key of the uploaded object Example: Read and upload a file to an S3 bucket: ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.s3 import s3_upload @flow async def example_s3_upload_flow(): aws_credentials = AwsCredentials( aws_access_key_id=\"acccess_key_id\", aws_secret_access_key=\"secret_access_key\" ) with open(\"data.csv\", \"rb\") as file: key = await s3_upload( bucket=\"bucket\", key=\"data.csv\", data=file.read(), aws_credentials=aws_credentials, ) example_s3_upload_flow() ``` \"\"\" logger = get_run_logger () key = key or str ( uuid . uuid4 ()) logger . info ( \"Uploading object to bucket %s with key %s \" , bucket , key ) s3_client = aws_credentials . get_boto3_session () . client ( \"s3\" , ** aws_client_parameters . get_params_override () ) stream = io . BytesIO ( data ) await run_sync_in_worker_thread ( s3_client . upload_fileobj , stream , Bucket = bucket , Key = key ) return key","title":"S3"},{"location":"s3/#prefect_aws.s3","text":"Tasks for interacting with AWS S3","title":"s3"},{"location":"s3/#prefect_aws.s3.S3Bucket","text":"Block used to store data using AWS S3 or S3-compatible object storage like MinIO. Parameters: Name Type Description Default bucket_name Name of your bucket. required aws_credentials A block containing your credentials (choose this or minio_credentials). required minio_credentials A block containing your credentials (choose this or aws_credentials). required basepath Used when you don't want to read/write at base level. required endpoint_url Used for non-AWS configuration. When unspecified, defaults to AWS. required Example Load stored S3Bucket configuration: from prefect_aws.s3 import S3Bucket s3bucket_block = S3Bucket . load ( \"BLOCK_NAME\" ) Source code in prefect_aws/s3.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 class S3Bucket ( ReadableFileSystem , WritableFileSystem ): \"\"\" Block used to store data using AWS S3 or S3-compatible object storage like MinIO. Args: bucket_name: Name of your bucket. aws_credentials: A block containing your credentials (choose this or minio_credentials). minio_credentials: A block containing your credentials (choose this or aws_credentials). basepath: Used when you don't want to read/write at base level. endpoint_url: Used for non-AWS configuration. When unspecified, defaults to AWS. Example: Load stored S3Bucket configuration: ```python from prefect_aws.s3 import S3Bucket s3bucket_block = S3Bucket.load(\"BLOCK_NAME\") ``` \"\"\" # change _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/uPezmBzEv4moXKdQJ3YyL/a1f029b423cf67f474d1eee33c1463d7/pngwing.com.png?h=250\" # noqa _block_type_name = \"S3 Bucket\" bucket_name : str minio_credentials : Optional [ MinIOCredentials ] aws_credentials : Optional [ AwsCredentials ] basepath : Optional [ Path ] endpoint_url : Optional [ str ] @validator ( \"basepath\" , pre = True ) def cast_pathlib ( cls , value ): \"\"\" If basepath provided, it means we aren't writing to the root directory of the bucket. We need to ensure that it is a valid path. This is called when the S3Bucket block is instantiated. \"\"\" if isinstance ( value , Path ): return str ( value ) return value @root_validator ( pre = True ) def check_credentials ( cls , values ): \"\"\" Ensure exactly 1 of 2 optional credentials fields has been provided by user. \"\"\" minio_creds_exist = bool ( values . get ( \"minio_credentials\" )) aws_creds_exist = bool ( values . get ( \"aws_credentials\" )) # if both credentials fields provided if minio_creds_exist and aws_creds_exist : raise ValueError ( \"S3Bucket accepts a minio_credentials field or an\" \"aws_credentials field but not both.\" ) # if neither credentials fields provided if not minio_creds_exist and not aws_creds_exist : raise ValueError ( \"S3Bucket requires either a minio_credentials\" \"field or an aws_credentials field.\" ) return values def _resolve_path ( self , path : str ) -> Path : \"\"\" A helper function used in write_path to join `self.basepath` and `path`. Args: path: Name of the key, e.g. \"file1\". Each object in your bucket has a unique key (or key name). \"\"\" path = path or str ( uuid4 ()) # If basepath provided, it means we won't write to the root dir of # the bucket. So we need to add it on the front of the path. path = str ( Path ( self . basepath ) / path ) if self . basepath else path return path def _get_s3_client ( self ) -> boto3 . client : \"\"\" Authenticate MinIO credentials or AWS credentials and return an S3 client. This is a helper function called by read_path() or write_path(). \"\"\" if self . minio_credentials : s3_client = self . minio_credentials . get_boto3_session () . client ( service_name = \"s3\" , endpoint_url = self . endpoint_url ) elif self . aws_credentials : s3_client = self . aws_credentials . get_boto3_session () . client ( service_name = \"s3\" ) return s3_client async def read_path ( self , path : str ) -> bytes : \"\"\" Read specified path from S3 and return contents. Provide the entire path to the key in S3. Args: path: Entire path to (and including) the key. Example: Read \"subfolder/file1\" contents from an S3 bucket named \"bucket\": ```python from prefect_aws import AwsCredentials from prefect_aws.s3 import S3Bucket aws_creds = AwsCredentials( aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY ) s3_bucket_block = S3Bucket( bucket_name=\"bucket\", aws_credentials=aws_creds, basepath=\"subfolder\" ) key_contents = s3_bucket_block.read_path(path=\"subfolder/file1\") ``` \"\"\" return await run_sync_in_worker_thread ( self . _read_sync , path ) def _read_sync ( self , key : str ) -> bytes : \"\"\" Called by read_path(). Creates an S3 client and retrieves the contents from a specified path. \"\"\" s3_client = self . _get_s3_client () with io . BytesIO () as stream : s3_client . download_fileobj ( Bucket = self . bucket_name , Key = key , Fileobj = stream ) stream . seek ( 0 ) output = stream . read () return output async def write_path ( self , path : str , content : bytes ) -> str : \"\"\" Writes to an S3 bucket. Args: path: The key name. Each object in your bucket has a unique key (or key name). content: What you are uploading to S3. Example: Write data to the path `dogs/small_dogs/havanese` in an S3 Bucket: ```python from prefect_aws import MinioCredentials from prefect_aws.s3 import S3Bucket minio_creds = MinIOCredentials( minio_root_user = \"minioadmin\", minio_root_password = \"minioadmin\", ) s3_bucket_block = S3Bucket( bucket_name=\"bucket\", minio_credentials=minio_creds, basepath=\"dogs/smalldogs\", endpoint_url=\"http://localhost:9000\", ) s3_havanese_path = s3_bucket_block.write_path(path=\"havanese\", content=data) ``` \"\"\" path = self . _resolve_path ( path ) await run_sync_in_worker_thread ( self . _write_sync , path , content ) return path def _write_sync ( self , key : str , data : bytes ) -> None : \"\"\" Called by write_path(). Creates an S3 client and uploads a file object. \"\"\" s3_client = self . _get_s3_client () with io . BytesIO ( data ) as stream : s3_client . upload_fileobj ( Fileobj = stream , Bucket = self . bucket_name , Key = key )","title":"S3Bucket"},{"location":"s3/#prefect_aws.s3.S3Bucket.cast_pathlib","text":"If basepath provided, it means we aren't writing to the root directory of the bucket. We need to ensure that it is a valid path. This is called when the S3Bucket block is instantiated. Source code in prefect_aws/s3.py 266 267 268 269 270 271 272 273 274 275 276 277 @validator ( \"basepath\" , pre = True ) def cast_pathlib ( cls , value ): \"\"\" If basepath provided, it means we aren't writing to the root directory of the bucket. We need to ensure that it is a valid path. This is called when the S3Bucket block is instantiated. \"\"\" if isinstance ( value , Path ): return str ( value ) return value","title":"cast_pathlib()"},{"location":"s3/#prefect_aws.s3.S3Bucket.check_credentials","text":"Ensure exactly 1 of 2 optional credentials fields has been provided by user. Source code in prefect_aws/s3.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 @root_validator ( pre = True ) def check_credentials ( cls , values ): \"\"\" Ensure exactly 1 of 2 optional credentials fields has been provided by user. \"\"\" minio_creds_exist = bool ( values . get ( \"minio_credentials\" )) aws_creds_exist = bool ( values . get ( \"aws_credentials\" )) # if both credentials fields provided if minio_creds_exist and aws_creds_exist : raise ValueError ( \"S3Bucket accepts a minio_credentials field or an\" \"aws_credentials field but not both.\" ) # if neither credentials fields provided if not minio_creds_exist and not aws_creds_exist : raise ValueError ( \"S3Bucket requires either a minio_credentials\" \"field or an aws_credentials field.\" ) return values","title":"check_credentials()"},{"location":"s3/#prefect_aws.s3.S3Bucket.read_path","text":"Read specified path from S3 and return contents. Provide the entire path to the key in S3. Parameters: Name Type Description Default path str Entire path to (and including) the key. required Example Read \"subfolder/file1\" contents from an S3 bucket named \"bucket\": from prefect_aws import AwsCredentials from prefect_aws.s3 import S3Bucket aws_creds = AwsCredentials ( aws_access_key_id = AWS_ACCESS_KEY_ID , aws_secret_access_key = AWS_SECRET_ACCESS_KEY ) s3_bucket_block = S3Bucket ( bucket_name = \"bucket\" , aws_credentials = aws_creds , basepath = \"subfolder\" ) key_contents = s3_bucket_block . read_path ( path = \"subfolder/file1\" ) Source code in prefect_aws/s3.py 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 async def read_path ( self , path : str ) -> bytes : \"\"\" Read specified path from S3 and return contents. Provide the entire path to the key in S3. Args: path: Entire path to (and including) the key. Example: Read \"subfolder/file1\" contents from an S3 bucket named \"bucket\": ```python from prefect_aws import AwsCredentials from prefect_aws.s3 import S3Bucket aws_creds = AwsCredentials( aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY ) s3_bucket_block = S3Bucket( bucket_name=\"bucket\", aws_credentials=aws_creds, basepath=\"subfolder\" ) key_contents = s3_bucket_block.read_path(path=\"subfolder/file1\") ``` \"\"\" return await run_sync_in_worker_thread ( self . _read_sync , path )","title":"read_path()"},{"location":"s3/#prefect_aws.s3.S3Bucket.write_path","text":"Writes to an S3 bucket. Parameters: Name Type Description Default path str The key name. Each object in your bucket has a unique key (or key name). required content bytes What you are uploading to S3. required Example Write data to the path dogs/small_dogs/havanese in an S3 Bucket: from prefect_aws import MinioCredentials from prefect_aws.s3 import S3Bucket minio_creds = MinIOCredentials ( minio_root_user = \"minioadmin\" , minio_root_password = \"minioadmin\" , ) s3_bucket_block = S3Bucket ( bucket_name = \"bucket\" , minio_credentials = minio_creds , basepath = \"dogs/smalldogs\" , endpoint_url = \"http://localhost:9000\" , ) s3_havanese_path = s3_bucket_block . write_path ( path = \"havanese\" , content = data ) Source code in prefect_aws/s3.py 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 async def write_path ( self , path : str , content : bytes ) -> str : \"\"\" Writes to an S3 bucket. Args: path: The key name. Each object in your bucket has a unique key (or key name). content: What you are uploading to S3. Example: Write data to the path `dogs/small_dogs/havanese` in an S3 Bucket: ```python from prefect_aws import MinioCredentials from prefect_aws.s3 import S3Bucket minio_creds = MinIOCredentials( minio_root_user = \"minioadmin\", minio_root_password = \"minioadmin\", ) s3_bucket_block = S3Bucket( bucket_name=\"bucket\", minio_credentials=minio_creds, basepath=\"dogs/smalldogs\", endpoint_url=\"http://localhost:9000\", ) s3_havanese_path = s3_bucket_block.write_path(path=\"havanese\", content=data) ``` \"\"\" path = self . _resolve_path ( path ) await run_sync_in_worker_thread ( self . _write_sync , path , content ) return path","title":"write_path()"},{"location":"s3/#prefect_aws.s3.s3_download","text":"Downloads an object with a given key from a given S3 bucket. Parameters: Name Type Description Default bucket str Name of bucket to download object from. Required if a default value was not supplied when creating the task. required key str Key of object to download. Required if a default value was not supplied when creating the task. required aws_credentials AwsCredentials Credentials to use for authentication with AWS. required aws_client_parameters AwsClientParameters Custom parameter for the boto3 client initialization.. AwsClientParameters() Returns: Type Description bytes A bytes representation of the downloaded object. Example Download a file from an S3 bucket: from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.s3 import s3_download @flow async def example_s3_download_flow (): aws_credentials = AwsCredentials ( aws_access_key_id = \"acccess_key_id\" , aws_secret_access_key = \"secret_access_key\" ) data = await s3_download ( bucket = \"bucket\" , key = \"key\" , aws_credentials = aws_credentials , ) example_s3_download_flow () Source code in prefect_aws/s3.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 @task async def s3_download ( bucket : str , key : str , aws_credentials : AwsCredentials , aws_client_parameters : AwsClientParameters = AwsClientParameters (), ) -> bytes : \"\"\" Downloads an object with a given key from a given S3 bucket. Args: bucket: Name of bucket to download object from. Required if a default value was not supplied when creating the task. key: Key of object to download. Required if a default value was not supplied when creating the task. aws_credentials: Credentials to use for authentication with AWS. aws_client_parameters: Custom parameter for the boto3 client initialization.. Returns: A `bytes` representation of the downloaded object. Example: Download a file from an S3 bucket: ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.s3 import s3_download @flow async def example_s3_download_flow(): aws_credentials = AwsCredentials( aws_access_key_id=\"acccess_key_id\", aws_secret_access_key=\"secret_access_key\" ) data = await s3_download( bucket=\"bucket\", key=\"key\", aws_credentials=aws_credentials, ) example_s3_download_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Downloading object from bucket %s with key %s \" , bucket , key ) s3_client = aws_credentials . get_boto3_session () . client ( \"s3\" , ** aws_client_parameters . get_params_override () ) stream = io . BytesIO () await run_sync_in_worker_thread ( s3_client . download_fileobj , Bucket = bucket , Key = key , Fileobj = stream ) stream . seek ( 0 ) output = stream . read () return output","title":"s3_download()"},{"location":"s3/#prefect_aws.s3.s3_list_objects","text":"Lists details of objects in a given S3 bucket. Parameters: Name Type Description Default bucket str Name of bucket to list items from. Required if a default value was not supplied when creating the task. required aws_credentials AwsCredentials Credentials to use for authentication with AWS. required aws_client_parameters AwsClientParameters Custom parameter for the boto3 client initialization.. AwsClientParameters() prefix str Used to filter objects with keys starting with the specified prefix. '' delimiter str Character used to group keys of listed objects. '' page_size Optional [ int ] Number of objects to return in each request to the AWS API. None max_items Optional [ int ] Maximum number of objects that to be returned by task. None jmespath_query Optional [ str ] Query used to filter objects based on object attributes refer to the boto3 docs for more information on how to construct queries. None Returns: Type Description List [ Dict [ str , Any ]] A list of dictionaries containing information about the objects retrieved. Refer to the boto3 docs for an example response. Example List all objects in a bucket: from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.s3 import s3_list_objects @flow async def example_s3_list_objects_flow (): aws_credentials = AwsCredentials ( aws_access_key_id = \"acccess_key_id\" , aws_secret_access_key = \"secret_access_key\" ) objects = await s3_list_objects ( bucket = \"data_bucket\" , aws_credentials = aws_credentials ) example_s3_list_objects_flow () Source code in prefect_aws/s3.py 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 @task async def s3_list_objects ( bucket : str , aws_credentials : AwsCredentials , aws_client_parameters : AwsClientParameters = AwsClientParameters (), prefix : str = \"\" , delimiter : str = \"\" , page_size : Optional [ int ] = None , max_items : Optional [ int ] = None , jmespath_query : Optional [ str ] = None , ) -> List [ Dict [ str , Any ]]: \"\"\" Lists details of objects in a given S3 bucket. Args: bucket: Name of bucket to list items from. Required if a default value was not supplied when creating the task. aws_credentials: Credentials to use for authentication with AWS. aws_client_parameters: Custom parameter for the boto3 client initialization.. prefix: Used to filter objects with keys starting with the specified prefix. delimiter: Character used to group keys of listed objects. page_size: Number of objects to return in each request to the AWS API. max_items: Maximum number of objects that to be returned by task. jmespath_query: Query used to filter objects based on object attributes refer to the [boto3 docs](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/paginators.html#filtering-results-with-jmespath) for more information on how to construct queries. Returns: A list of dictionaries containing information about the objects retrieved. Refer to the boto3 docs for an example response. Example: List all objects in a bucket: ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.s3 import s3_list_objects @flow async def example_s3_list_objects_flow(): aws_credentials = AwsCredentials( aws_access_key_id=\"acccess_key_id\", aws_secret_access_key=\"secret_access_key\" ) objects = await s3_list_objects( bucket=\"data_bucket\", aws_credentials=aws_credentials ) example_s3_list_objects_flow() ``` \"\"\" # noqa E501 logger = get_run_logger () logger . info ( \"Listing objects in bucket %s with prefix %s \" , bucket , prefix ) s3_client = aws_credentials . get_boto3_session () . client ( \"s3\" , ** aws_client_parameters . get_params_override () ) paginator = s3_client . get_paginator ( \"list_objects_v2\" ) page_iterator = paginator . paginate ( Bucket = bucket , Prefix = prefix , Delimiter = delimiter , PaginationConfig = { \"PageSize\" : page_size , \"MaxItems\" : max_items }, ) if jmespath_query : page_iterator = page_iterator . search ( f \" { jmespath_query } | {{ Contents: @ }} \" ) return await run_sync_in_worker_thread ( _list_objects_sync , page_iterator )","title":"s3_list_objects()"},{"location":"s3/#prefect_aws.s3.s3_upload","text":"Uploads data to an S3 bucket. Parameters: Name Type Description Default data bytes Bytes representation of data to upload to S3. required bucket str Name of bucket to upload data to. Required if a default value was not supplied when creating the task. required aws_credentials AwsCredentials Credentials to use for authentication with AWS. required aws_client_parameters AwsClientParameters Custom parameter for the boto3 client initialization.. AwsClientParameters() key Optional [ str ] Key of object to download. Defaults to a UUID string. None Returns: Type Description str The key of the uploaded object Example Read and upload a file to an S3 bucket: from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.s3 import s3_upload @flow async def example_s3_upload_flow (): aws_credentials = AwsCredentials ( aws_access_key_id = \"acccess_key_id\" , aws_secret_access_key = \"secret_access_key\" ) with open ( \"data.csv\" , \"rb\" ) as file : key = await s3_upload ( bucket = \"bucket\" , key = \"data.csv\" , data = file . read (), aws_credentials = aws_credentials , ) example_s3_upload_flow () Source code in prefect_aws/s3.py 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 @task async def s3_upload ( data : bytes , bucket : str , aws_credentials : AwsCredentials , aws_client_parameters : AwsClientParameters = AwsClientParameters (), key : Optional [ str ] = None , ) -> str : \"\"\" Uploads data to an S3 bucket. Args: data: Bytes representation of data to upload to S3. bucket: Name of bucket to upload data to. Required if a default value was not supplied when creating the task. aws_credentials: Credentials to use for authentication with AWS. aws_client_parameters: Custom parameter for the boto3 client initialization.. key: Key of object to download. Defaults to a UUID string. Returns: The key of the uploaded object Example: Read and upload a file to an S3 bucket: ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.s3 import s3_upload @flow async def example_s3_upload_flow(): aws_credentials = AwsCredentials( aws_access_key_id=\"acccess_key_id\", aws_secret_access_key=\"secret_access_key\" ) with open(\"data.csv\", \"rb\") as file: key = await s3_upload( bucket=\"bucket\", key=\"data.csv\", data=file.read(), aws_credentials=aws_credentials, ) example_s3_upload_flow() ``` \"\"\" logger = get_run_logger () key = key or str ( uuid . uuid4 ()) logger . info ( \"Uploading object to bucket %s with key %s \" , bucket , key ) s3_client = aws_credentials . get_boto3_session () . client ( \"s3\" , ** aws_client_parameters . get_params_override () ) stream = io . BytesIO ( data ) await run_sync_in_worker_thread ( s3_client . upload_fileobj , stream , Bucket = bucket , Key = key ) return key","title":"s3_upload()"},{"location":"secrets_manager/","text":"prefect_aws.secrets_manager Tasks for interacting with AWS Secrets Manager create_secret async Creates a secret in AWS Secrets Manager. Parameters: Name Type Description Default secret_name str The name of the secret to create. required secret_value Union [ str , bytes ] The value to store in the created secret. required aws_credentials AwsCredentials Credentials to use for authentication with AWS. required description Optional [ str ] A description for the created secret. None tags Optional [ List [ Dict [ str , str ]]] A list of tags to attach to the secret. Each tag should be specified as a dictionary in the following format: { \"Key\" : str , \"Value\" : str } None Returns: Type Description Dict [ str , str ] Dict[str, str]: A dict containing the secret ARN (Amazon Resource Name), name, and current version ID. { \"ARN\" : str , \"Name\" : str , \"VersionId\" : str } Example Create a secret: from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import create_secret @flow def example_create_secret (): aws_credentials = AwsCredentials ( aws_access_key_id = \"access_key_id\" , aws_secret_access_key = \"secret_access_key\" ) create_secret ( secret_name = \"life_the_universe_and_everything\" , secret_value = \"42\" , aws_credentials = aws_credentials ) example_create_secret () Source code in prefect_aws/secrets_manager.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 @task async def create_secret ( secret_name : str , secret_value : Union [ str , bytes ], aws_credentials : AwsCredentials , description : Optional [ str ] = None , tags : Optional [ List [ Dict [ str , str ]]] = None , ) -> Dict [ str , str ]: \"\"\" Creates a secret in AWS Secrets Manager. Args: secret_name: The name of the secret to create. secret_value: The value to store in the created secret. aws_credentials: Credentials to use for authentication with AWS. description: A description for the created secret. tags: A list of tags to attach to the secret. Each tag should be specified as a dictionary in the following format: ```python { \"Key\": str, \"Value\": str } ``` Returns: Dict[str, str]: A dict containing the secret ARN (Amazon Resource Name), name, and current version ID. ```python { \"ARN\": str, \"Name\": str, \"VersionId\": str } ``` Example: Create a secret: ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import create_secret @flow def example_create_secret(): aws_credentials = AwsCredentials( aws_access_key_id=\"access_key_id\", aws_secret_access_key=\"secret_access_key\" ) create_secret( secret_name=\"life_the_universe_and_everything\", secret_value=\"42\", aws_credentials=aws_credentials ) example_create_secret() ``` \"\"\" create_secret_kwargs : Dict [ str , Union [ str , bytes , List [ Dict [ str , str ]]]] = dict ( Name = secret_name ) if description is not None : create_secret_kwargs [ \"Description\" ] = description if tags is not None : create_secret_kwargs [ \"Tags\" ] = tags if isinstance ( secret_value , bytes ): create_secret_kwargs [ \"SecretBinary\" ] = secret_value elif isinstance ( secret_value , str ): create_secret_kwargs [ \"SecretString\" ] = secret_value else : raise ValueError ( \"Please provide a bytes or str value for secret_value\" ) logger = get_run_logger () logger . info ( \"Creating secret named %s \" , secret_name ) client = aws_credentials . get_boto3_session () . client ( \"secretsmanager\" ) try : response = await run_sync_in_worker_thread ( client . create_secret , ** create_secret_kwargs ) print ( response . pop ( \"ResponseMetadata\" , None )) return response except ClientError : logger . exception ( \"Unable to create secret %s \" , secret_name ) raise delete_secret async Deletes a secret from AWS Secrets Manager. Secrets can either be deleted immediately by setting force_delete_without_recovery equal to True . Otherwise, secrets will be marked for deletion and available for recovery for the number of days specified in recovery_window_in_days Parameters: Name Type Description Default secret_name str Name of the secret to be deleted. required aws_credentials AwsCredentials Credentials to use for authentication with AWS. required recovery_window_in_days int Number of days a secret should be recoverable for before permenant deletion. Minium window is 7 days and maximum window is 30 days. If force_delete_without_recovery is set to True , this value will be ignored. 30 force_delete_without_recovery bool If True , the secret will be immediately deleted and will not be recoverable. False Returns: Type Description Dict[str, str]: A dict containing the secret ARN (Amazon Resource Name), name, and deletion date of the secret. DeletionDate is the date and time of the delete request plus the number of days in recovery_window_in_days . { \"ARN\" : str , \"Name\" : str , \"DeletionDate\" : datetime . datetime } Examples: Delete a secret immediately: from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import delete_secret @flow def example_delete_secret_immediately (): aws_credentials = AwsCredentials ( aws_access_key_id = \"access_key_id\" , aws_secret_access_key = \"secret_access_key\" ) delete_secret ( secret_name = \"life_the_universe_and_everything\" , aws_credentials = aws_credentials , force_delete_without_recovery : True ) example_delete_secret_immediately () Delete a secret with a 90 day recovery window: from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import delete_secret @flow def example_delete_secret_with_recovery_window (): aws_credentials = AwsCredentials ( aws_access_key_id = \"access_key_id\" , aws_secret_access_key = \"secret_access_key\" ) delete_secret ( secret_name = \"life_the_universe_and_everything\" , aws_credentials = aws_credentials , recovery_window_in_days = 90 ) example_delete_secret_with_recovery_window () Source code in prefect_aws/secrets_manager.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 @task async def delete_secret ( secret_name : str , aws_credentials : AwsCredentials , recovery_window_in_days : int = 30 , force_delete_without_recovery : bool = False , ): \"\"\" Deletes a secret from AWS Secrets Manager. Secrets can either be deleted immediately by setting `force_delete_without_recovery` equal to `True`. Otherwise, secrets will be marked for deletion and available for recovery for the number of days specified in `recovery_window_in_days` Args: secret_name: Name of the secret to be deleted. aws_credentials: Credentials to use for authentication with AWS. recovery_window_in_days: Number of days a secret should be recoverable for before permenant deletion. Minium window is 7 days and maximum window is 30 days. If `force_delete_without_recovery` is set to `True`, this value will be ignored. force_delete_without_recovery: If `True`, the secret will be immediately deleted and will not be recoverable. Returns: Dict[str, str]: A dict containing the secret ARN (Amazon Resource Name), name, and deletion date of the secret. DeletionDate is the date and time of the delete request plus the number of days in `recovery_window_in_days`. ```python { \"ARN\": str, \"Name\": str, \"DeletionDate\": datetime.datetime } ``` Examples: Delete a secret immediately: ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import delete_secret @flow def example_delete_secret_immediately(): aws_credentials = AwsCredentials( aws_access_key_id=\"access_key_id\", aws_secret_access_key=\"secret_access_key\" ) delete_secret( secret_name=\"life_the_universe_and_everything\", aws_credentials=aws_credentials, force_delete_without_recovery: True ) example_delete_secret_immediately() ``` Delete a secret with a 90 day recovery window: ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import delete_secret @flow def example_delete_secret_with_recovery_window(): aws_credentials = AwsCredentials( aws_access_key_id=\"access_key_id\", aws_secret_access_key=\"secret_access_key\" ) delete_secret( secret_name=\"life_the_universe_and_everything\", aws_credentials=aws_credentials, recovery_window_in_days=90 ) example_delete_secret_with_recovery_window() ``` \"\"\" if not force_delete_without_recovery and not ( 7 <= recovery_window_in_days <= 30 ): raise ValueError ( \"Recovery window must be between 7 and 30 days.\" ) delete_secret_kwargs : Dict [ str , Union [ str , int , bool ]] = dict ( SecretId = secret_name ) if force_delete_without_recovery : delete_secret_kwargs [ \"ForceDeleteWithoutRecovery\" ] = force_delete_without_recovery else : delete_secret_kwargs [ \"RecoveryWindowInDays\" ] = recovery_window_in_days logger = get_run_logger () logger . info ( \"Deleting secret %s \" , secret_name ) client = aws_credentials . get_boto3_session () . client ( \"secretsmanager\" ) try : response = await run_sync_in_worker_thread ( client . delete_secret , ** delete_secret_kwargs ) response . pop ( \"ResponseMetadata\" , None ) return response except ClientError : logger . exception ( \"Unable to delete secret %s \" , secret_name ) raise read_secret async Reads the value of a given secret from AWS Secrets Manager. Parameters: Name Type Description Default secret_name str Name of stored secret. required aws_credentials AwsCredentials Credentials to use for authentication with AWS. required version_id Optional [ str ] Specifies version of secret to read. Defaults to the most recent version if not given. None version_stage Optional [ str ] Specifies the version stage of the secret to read. Defaults to AWS_CURRENT if not given. None Returns: Type Description Union [ str , bytes ] The secret values as a str or bytes depending on the format in which the secret was stored. Example Read a secret value: from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import read_secret @flow def example_read_secret (): aws_credentials = AwsCredentials ( aws_access_key_id = \"access_key_id\" , aws_secret_access_key = \"secret_access_key\" ) secret_value = read_secret ( secret_name = \"db_password\" , aws_credentials = aws_credentials ) example_read_secret () Source code in prefect_aws/secrets_manager.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 @task async def read_secret ( secret_name : str , aws_credentials : AwsCredentials , version_id : Optional [ str ] = None , version_stage : Optional [ str ] = None , ) -> Union [ str , bytes ]: \"\"\" Reads the value of a given secret from AWS Secrets Manager. Args: secret_name: Name of stored secret. aws_credentials: Credentials to use for authentication with AWS. version_id: Specifies version of secret to read. Defaults to the most recent version if not given. version_stage: Specifies the version stage of the secret to read. Defaults to AWS_CURRENT if not given. Returns: The secret values as a `str` or `bytes` depending on the format in which the secret was stored. Example: Read a secret value: ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import read_secret @flow def example_read_secret(): aws_credentials = AwsCredentials( aws_access_key_id=\"access_key_id\", aws_secret_access_key=\"secret_access_key\" ) secret_value = read_secret( secret_name=\"db_password\", aws_credentials=aws_credentials ) example_read_secret() ``` \"\"\" logger = get_run_logger () logger . info ( \"Getting value for secret %s \" , secret_name ) client = aws_credentials . get_boto3_session () . client ( \"secretsmanager\" ) get_secret_value_kwargs = dict ( SecretId = secret_name ) if version_id is not None : get_secret_value_kwargs [ \"VersionId\" ] = version_id if version_stage is not None : get_secret_value_kwargs [ \"VersionStage\" ] = version_stage try : response = await run_sync_in_worker_thread ( client . get_secret_value , ** get_secret_value_kwargs ) except ClientError : logger . exception ( \"Unable to get value for secret %s \" , secret_name ) raise else : return response . get ( \"SecretString\" ) or response . get ( \"SecretBinary\" ) update_secret async Updates the value of a given secret in AWS Secrets Manager. Parameters: Name Type Description Default secret_name str Name of secret to update. required secret_value Union [ str , bytes ] Desired value of the secret. Can be either str or bytes . required aws_credentials AwsCredentials Credentials to use for authentication with AWS. required description Optional [ str ] Desired description of the secret. None Returns: Type Description Dict [ str , str ] Dict[str, str]: A dict containing the secret ARN (Amazon Resource Name), name, and current version ID. { \"ARN\" : str , \"Name\" : str , \"VersionId\" : str } Example Update a secret value: from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import update_secret @flow def example_update_secret (): aws_credentials = AwsCredentials ( aws_access_key_id = \"access_key_id\" , aws_secret_access_key = \"secret_access_key\" ) update_secret ( secret_name = \"life_the_universe_and_everything\" , secret_value = \"42\" , aws_credentials = aws_credentials ) example_update_secret () Source code in prefect_aws/secrets_manager.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 @task async def update_secret ( secret_name : str , secret_value : Union [ str , bytes ], aws_credentials : AwsCredentials , description : Optional [ str ] = None , ) -> Dict [ str , str ]: \"\"\" Updates the value of a given secret in AWS Secrets Manager. Args: secret_name: Name of secret to update. secret_value: Desired value of the secret. Can be either `str` or `bytes`. aws_credentials: Credentials to use for authentication with AWS. description: Desired description of the secret. Returns: Dict[str, str]: A dict containing the secret ARN (Amazon Resource Name), name, and current version ID. ```python { \"ARN\": str, \"Name\": str, \"VersionId\": str } ``` Example: Update a secret value: ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import update_secret @flow def example_update_secret(): aws_credentials = AwsCredentials( aws_access_key_id=\"access_key_id\", aws_secret_access_key=\"secret_access_key\" ) update_secret( secret_name=\"life_the_universe_and_everything\", secret_value=\"42\", aws_credentials=aws_credentials ) example_update_secret() ``` \"\"\" update_secret_kwargs : Dict [ str , Union [ str , bytes ]] = dict ( SecretId = secret_name ) if description is not None : update_secret_kwargs [ \"Description\" ] = description if isinstance ( secret_value , bytes ): update_secret_kwargs [ \"SecretBinary\" ] = secret_value elif isinstance ( secret_value , str ): update_secret_kwargs [ \"SecretString\" ] = secret_value else : raise ValueError ( \"Please provide a bytes or str value for secret_value\" ) logger = get_run_logger () logger . info ( \"Updating value for secret %s \" , secret_name ) client = aws_credentials . get_boto3_session () . client ( \"secretsmanager\" ) try : response = await run_sync_in_worker_thread ( client . update_secret , ** update_secret_kwargs ) response . pop ( \"ResponseMetadata\" , None ) return response except ClientError : logger . exception ( \"Unable to update secret %s \" , secret_name ) raise","title":"Secrets Manager"},{"location":"secrets_manager/#prefect_aws.secrets_manager","text":"Tasks for interacting with AWS Secrets Manager","title":"secrets_manager"},{"location":"secrets_manager/#prefect_aws.secrets_manager.create_secret","text":"Creates a secret in AWS Secrets Manager. Parameters: Name Type Description Default secret_name str The name of the secret to create. required secret_value Union [ str , bytes ] The value to store in the created secret. required aws_credentials AwsCredentials Credentials to use for authentication with AWS. required description Optional [ str ] A description for the created secret. None tags Optional [ List [ Dict [ str , str ]]] A list of tags to attach to the secret. Each tag should be specified as a dictionary in the following format: { \"Key\" : str , \"Value\" : str } None Returns: Type Description Dict [ str , str ] Dict[str, str]: A dict containing the secret ARN (Amazon Resource Name), name, and current version ID. { \"ARN\" : str , \"Name\" : str , \"VersionId\" : str } Example Create a secret: from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import create_secret @flow def example_create_secret (): aws_credentials = AwsCredentials ( aws_access_key_id = \"access_key_id\" , aws_secret_access_key = \"secret_access_key\" ) create_secret ( secret_name = \"life_the_universe_and_everything\" , secret_value = \"42\" , aws_credentials = aws_credentials ) example_create_secret () Source code in prefect_aws/secrets_manager.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 @task async def create_secret ( secret_name : str , secret_value : Union [ str , bytes ], aws_credentials : AwsCredentials , description : Optional [ str ] = None , tags : Optional [ List [ Dict [ str , str ]]] = None , ) -> Dict [ str , str ]: \"\"\" Creates a secret in AWS Secrets Manager. Args: secret_name: The name of the secret to create. secret_value: The value to store in the created secret. aws_credentials: Credentials to use for authentication with AWS. description: A description for the created secret. tags: A list of tags to attach to the secret. Each tag should be specified as a dictionary in the following format: ```python { \"Key\": str, \"Value\": str } ``` Returns: Dict[str, str]: A dict containing the secret ARN (Amazon Resource Name), name, and current version ID. ```python { \"ARN\": str, \"Name\": str, \"VersionId\": str } ``` Example: Create a secret: ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import create_secret @flow def example_create_secret(): aws_credentials = AwsCredentials( aws_access_key_id=\"access_key_id\", aws_secret_access_key=\"secret_access_key\" ) create_secret( secret_name=\"life_the_universe_and_everything\", secret_value=\"42\", aws_credentials=aws_credentials ) example_create_secret() ``` \"\"\" create_secret_kwargs : Dict [ str , Union [ str , bytes , List [ Dict [ str , str ]]]] = dict ( Name = secret_name ) if description is not None : create_secret_kwargs [ \"Description\" ] = description if tags is not None : create_secret_kwargs [ \"Tags\" ] = tags if isinstance ( secret_value , bytes ): create_secret_kwargs [ \"SecretBinary\" ] = secret_value elif isinstance ( secret_value , str ): create_secret_kwargs [ \"SecretString\" ] = secret_value else : raise ValueError ( \"Please provide a bytes or str value for secret_value\" ) logger = get_run_logger () logger . info ( \"Creating secret named %s \" , secret_name ) client = aws_credentials . get_boto3_session () . client ( \"secretsmanager\" ) try : response = await run_sync_in_worker_thread ( client . create_secret , ** create_secret_kwargs ) print ( response . pop ( \"ResponseMetadata\" , None )) return response except ClientError : logger . exception ( \"Unable to create secret %s \" , secret_name ) raise","title":"create_secret()"},{"location":"secrets_manager/#prefect_aws.secrets_manager.delete_secret","text":"Deletes a secret from AWS Secrets Manager. Secrets can either be deleted immediately by setting force_delete_without_recovery equal to True . Otherwise, secrets will be marked for deletion and available for recovery for the number of days specified in recovery_window_in_days Parameters: Name Type Description Default secret_name str Name of the secret to be deleted. required aws_credentials AwsCredentials Credentials to use for authentication with AWS. required recovery_window_in_days int Number of days a secret should be recoverable for before permenant deletion. Minium window is 7 days and maximum window is 30 days. If force_delete_without_recovery is set to True , this value will be ignored. 30 force_delete_without_recovery bool If True , the secret will be immediately deleted and will not be recoverable. False Returns: Type Description Dict[str, str]: A dict containing the secret ARN (Amazon Resource Name), name, and deletion date of the secret. DeletionDate is the date and time of the delete request plus the number of days in recovery_window_in_days . { \"ARN\" : str , \"Name\" : str , \"DeletionDate\" : datetime . datetime } Examples: Delete a secret immediately: from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import delete_secret @flow def example_delete_secret_immediately (): aws_credentials = AwsCredentials ( aws_access_key_id = \"access_key_id\" , aws_secret_access_key = \"secret_access_key\" ) delete_secret ( secret_name = \"life_the_universe_and_everything\" , aws_credentials = aws_credentials , force_delete_without_recovery : True ) example_delete_secret_immediately () Delete a secret with a 90 day recovery window: from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import delete_secret @flow def example_delete_secret_with_recovery_window (): aws_credentials = AwsCredentials ( aws_access_key_id = \"access_key_id\" , aws_secret_access_key = \"secret_access_key\" ) delete_secret ( secret_name = \"life_the_universe_and_everything\" , aws_credentials = aws_credentials , recovery_window_in_days = 90 ) example_delete_secret_with_recovery_window () Source code in prefect_aws/secrets_manager.py 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 @task async def delete_secret ( secret_name : str , aws_credentials : AwsCredentials , recovery_window_in_days : int = 30 , force_delete_without_recovery : bool = False , ): \"\"\" Deletes a secret from AWS Secrets Manager. Secrets can either be deleted immediately by setting `force_delete_without_recovery` equal to `True`. Otherwise, secrets will be marked for deletion and available for recovery for the number of days specified in `recovery_window_in_days` Args: secret_name: Name of the secret to be deleted. aws_credentials: Credentials to use for authentication with AWS. recovery_window_in_days: Number of days a secret should be recoverable for before permenant deletion. Minium window is 7 days and maximum window is 30 days. If `force_delete_without_recovery` is set to `True`, this value will be ignored. force_delete_without_recovery: If `True`, the secret will be immediately deleted and will not be recoverable. Returns: Dict[str, str]: A dict containing the secret ARN (Amazon Resource Name), name, and deletion date of the secret. DeletionDate is the date and time of the delete request plus the number of days in `recovery_window_in_days`. ```python { \"ARN\": str, \"Name\": str, \"DeletionDate\": datetime.datetime } ``` Examples: Delete a secret immediately: ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import delete_secret @flow def example_delete_secret_immediately(): aws_credentials = AwsCredentials( aws_access_key_id=\"access_key_id\", aws_secret_access_key=\"secret_access_key\" ) delete_secret( secret_name=\"life_the_universe_and_everything\", aws_credentials=aws_credentials, force_delete_without_recovery: True ) example_delete_secret_immediately() ``` Delete a secret with a 90 day recovery window: ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import delete_secret @flow def example_delete_secret_with_recovery_window(): aws_credentials = AwsCredentials( aws_access_key_id=\"access_key_id\", aws_secret_access_key=\"secret_access_key\" ) delete_secret( secret_name=\"life_the_universe_and_everything\", aws_credentials=aws_credentials, recovery_window_in_days=90 ) example_delete_secret_with_recovery_window() ``` \"\"\" if not force_delete_without_recovery and not ( 7 <= recovery_window_in_days <= 30 ): raise ValueError ( \"Recovery window must be between 7 and 30 days.\" ) delete_secret_kwargs : Dict [ str , Union [ str , int , bool ]] = dict ( SecretId = secret_name ) if force_delete_without_recovery : delete_secret_kwargs [ \"ForceDeleteWithoutRecovery\" ] = force_delete_without_recovery else : delete_secret_kwargs [ \"RecoveryWindowInDays\" ] = recovery_window_in_days logger = get_run_logger () logger . info ( \"Deleting secret %s \" , secret_name ) client = aws_credentials . get_boto3_session () . client ( \"secretsmanager\" ) try : response = await run_sync_in_worker_thread ( client . delete_secret , ** delete_secret_kwargs ) response . pop ( \"ResponseMetadata\" , None ) return response except ClientError : logger . exception ( \"Unable to delete secret %s \" , secret_name ) raise","title":"delete_secret()"},{"location":"secrets_manager/#prefect_aws.secrets_manager.read_secret","text":"Reads the value of a given secret from AWS Secrets Manager. Parameters: Name Type Description Default secret_name str Name of stored secret. required aws_credentials AwsCredentials Credentials to use for authentication with AWS. required version_id Optional [ str ] Specifies version of secret to read. Defaults to the most recent version if not given. None version_stage Optional [ str ] Specifies the version stage of the secret to read. Defaults to AWS_CURRENT if not given. None Returns: Type Description Union [ str , bytes ] The secret values as a str or bytes depending on the format in which the secret was stored. Example Read a secret value: from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import read_secret @flow def example_read_secret (): aws_credentials = AwsCredentials ( aws_access_key_id = \"access_key_id\" , aws_secret_access_key = \"secret_access_key\" ) secret_value = read_secret ( secret_name = \"db_password\" , aws_credentials = aws_credentials ) example_read_secret () Source code in prefect_aws/secrets_manager.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 @task async def read_secret ( secret_name : str , aws_credentials : AwsCredentials , version_id : Optional [ str ] = None , version_stage : Optional [ str ] = None , ) -> Union [ str , bytes ]: \"\"\" Reads the value of a given secret from AWS Secrets Manager. Args: secret_name: Name of stored secret. aws_credentials: Credentials to use for authentication with AWS. version_id: Specifies version of secret to read. Defaults to the most recent version if not given. version_stage: Specifies the version stage of the secret to read. Defaults to AWS_CURRENT if not given. Returns: The secret values as a `str` or `bytes` depending on the format in which the secret was stored. Example: Read a secret value: ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import read_secret @flow def example_read_secret(): aws_credentials = AwsCredentials( aws_access_key_id=\"access_key_id\", aws_secret_access_key=\"secret_access_key\" ) secret_value = read_secret( secret_name=\"db_password\", aws_credentials=aws_credentials ) example_read_secret() ``` \"\"\" logger = get_run_logger () logger . info ( \"Getting value for secret %s \" , secret_name ) client = aws_credentials . get_boto3_session () . client ( \"secretsmanager\" ) get_secret_value_kwargs = dict ( SecretId = secret_name ) if version_id is not None : get_secret_value_kwargs [ \"VersionId\" ] = version_id if version_stage is not None : get_secret_value_kwargs [ \"VersionStage\" ] = version_stage try : response = await run_sync_in_worker_thread ( client . get_secret_value , ** get_secret_value_kwargs ) except ClientError : logger . exception ( \"Unable to get value for secret %s \" , secret_name ) raise else : return response . get ( \"SecretString\" ) or response . get ( \"SecretBinary\" )","title":"read_secret()"},{"location":"secrets_manager/#prefect_aws.secrets_manager.update_secret","text":"Updates the value of a given secret in AWS Secrets Manager. Parameters: Name Type Description Default secret_name str Name of secret to update. required secret_value Union [ str , bytes ] Desired value of the secret. Can be either str or bytes . required aws_credentials AwsCredentials Credentials to use for authentication with AWS. required description Optional [ str ] Desired description of the secret. None Returns: Type Description Dict [ str , str ] Dict[str, str]: A dict containing the secret ARN (Amazon Resource Name), name, and current version ID. { \"ARN\" : str , \"Name\" : str , \"VersionId\" : str } Example Update a secret value: from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import update_secret @flow def example_update_secret (): aws_credentials = AwsCredentials ( aws_access_key_id = \"access_key_id\" , aws_secret_access_key = \"secret_access_key\" ) update_secret ( secret_name = \"life_the_universe_and_everything\" , secret_value = \"42\" , aws_credentials = aws_credentials ) example_update_secret () Source code in prefect_aws/secrets_manager.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 @task async def update_secret ( secret_name : str , secret_value : Union [ str , bytes ], aws_credentials : AwsCredentials , description : Optional [ str ] = None , ) -> Dict [ str , str ]: \"\"\" Updates the value of a given secret in AWS Secrets Manager. Args: secret_name: Name of secret to update. secret_value: Desired value of the secret. Can be either `str` or `bytes`. aws_credentials: Credentials to use for authentication with AWS. description: Desired description of the secret. Returns: Dict[str, str]: A dict containing the secret ARN (Amazon Resource Name), name, and current version ID. ```python { \"ARN\": str, \"Name\": str, \"VersionId\": str } ``` Example: Update a secret value: ```python from prefect import flow from prefect_aws import AwsCredentials from prefect_aws.secrets_manager import update_secret @flow def example_update_secret(): aws_credentials = AwsCredentials( aws_access_key_id=\"access_key_id\", aws_secret_access_key=\"secret_access_key\" ) update_secret( secret_name=\"life_the_universe_and_everything\", secret_value=\"42\", aws_credentials=aws_credentials ) example_update_secret() ``` \"\"\" update_secret_kwargs : Dict [ str , Union [ str , bytes ]] = dict ( SecretId = secret_name ) if description is not None : update_secret_kwargs [ \"Description\" ] = description if isinstance ( secret_value , bytes ): update_secret_kwargs [ \"SecretBinary\" ] = secret_value elif isinstance ( secret_value , str ): update_secret_kwargs [ \"SecretString\" ] = secret_value else : raise ValueError ( \"Please provide a bytes or str value for secret_value\" ) logger = get_run_logger () logger . info ( \"Updating value for secret %s \" , secret_name ) client = aws_credentials . get_boto3_session () . client ( \"secretsmanager\" ) try : response = await run_sync_in_worker_thread ( client . update_secret , ** update_secret_kwargs ) response . pop ( \"ResponseMetadata\" , None ) return response except ClientError : logger . exception ( \"Unable to update secret %s \" , secret_name ) raise","title":"update_secret()"}]}